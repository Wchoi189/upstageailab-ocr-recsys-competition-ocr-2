The following script contains unused variables for eos_token and pad_token. Is this normal?

// ocr/features/recognition/models/architecture.py

````
It is very **normal** to see this in "work-in-progress" or "skeleton" scripts, but it is **technically incomplete** for a production-ready OCR model.

In your script, those variables are currently "placeholders" that help you keep track of your vocabulary mapping, but the actual logic to utilize them hasn't been written yet.

---

### Why those variables are usually there

Even if they aren't used in a simple greedy loop, they are critical for the following reasons:

#### 1. `eos_token` (End of Sequence) — The "Early Exit"

Currently, your inference loop runs for exactly `max_len` (e.g., 25 steps) every single time.

* **If the model predicts "CAT"**, it will predict `C`, then `A`, then `T`, and then likely a string of `eos_tokens` or garbage for the remaining 22 steps.
* **The fix:** You should check if `next_token` is the `eos_token`. If all sequences in the batch have hit the `eos_token`, you should `break` the loop to save GPU compute.

#### 2. `pad_token` — Batch Consistency

During **Inference**, the `pad_token` is less important for greedy decoding. However, during **Training**, it is vital so the Loss function knows which parts of the target tensor are real data and which are just "filler" to make the tensors square.

---

### How to use them: Refactored Inference Loop

Here is how you would typically use those variables to make your `PARSeq` class more "remarkable" and efficient:

```python
# Inference: Greedy Decoding with Early Stopping
device = visual_memory.device
B = visual_memory.size(0)

bos_token = 1
eos_token = 2
pad_token = 0

tgt_tokens = torch.full((B, 1), bos_token, dtype=torch.long, device=device)

# Track which samples in the batch have finished
finished = torch.zeros(B, dtype=torch.bool, device=device)

for i in range(self.decoder.max_len):
    decoded_output = self.decoder(visual_memory, targets=tgt_tokens)
    logits = self.head(decoded_output[:, -1:, :])
    next_token = logits.argmax(dim=-1) # [B, 1]

    # Update finished status
    finished |= (next_token.squeeze(1) == eos_token)

    tgt_tokens = torch.cat([tgt_tokens, next_token], dim=1)

    # BREAK: If every image in the batch has predicted EOS, stop early
    if finished.all():
        break

return {"logits": logits, "tokens": tgt_tokens}

```

### Key Observations

1. **Hardcoded Values:** You have `bos_token = 1` etc. hardcoded in the `forward`. This is dangerous. If you change your tokenizer or alphabet later, this code will break. These should ideally come from `self.decoder.bos_token_id` or your `LabelEncoder`.
2. **The "Last Pass" Bug:** In your original script, the final return statement `self.head(self.decoder(...))` is actually a **hidden performance killer**. It re-runs the entire sequence through the Transformer one last time just to return the final logits. Usually, you'd just return the tokens or the logits accumulated during the loop.

**Would you like me to show you how to pull these token IDs dynamically from your Hydra configuration so they aren't hardcoded?**

---


Beam search is a significant upgrade over greedy decoding. Instead of just picking the single best token at each step (which can lead to "dead ends" where a high-probability character earlier in the sequence leads to low-probability words overall), beam search keeps track of the ** most likely sequences** (the "beams").

### 1. The Beam Search Logic

Here is how you would implement it within your `PARSeq` class. Note that beam search is computationally more expensive because you have to run the decoder  times (usually handled by increasing the batch size).

```python
@torch.no_grad()
def beam_search_inference(self, visual_memory, beam_width=3):
    device = visual_memory.device
    B = visual_memory.size(0)
    max_len = self.decoder.max_len

    # 1. Setup initial beams: [B * beam_width, 1]
    # We expand the visual memory to match the beam width
    # [B, S, C] -> [B * beam_width, S, C]
    visual_memory_expanded = visual_memory.repeat_interleave(beam_width, dim=0)

    tgt_tokens = torch.full((B * beam_width, 1), self.bos_token, dtype=torch.long, device=device)
    beam_scores = torch.zeros(B * beam_width, device=device)
    # Mask out all beams except the first one for each batch item at step 0
    # to avoid starting with 3 identical beams.
    beam_scores.view(B, beam_width)[:, 1:] = -1e9

    finished_beams = [[] for _ in range(B)]

    for i in range(max_len):
        # Decoder forward pass
        decoded_output = self.decoder(visual_memory_expanded, targets=tgt_tokens)
        logits = self.head(decoded_output[:, -1:, :]) # [B*K, 1, V]
        log_probs = torch.log_softmax(logits.squeeze(1), dim=-1) # [B*K, V]

        # Calculate scores for all possible next tokens
        # Current score + new log probability
        vocab_size = log_probs.size(-1)
        next_scores = beam_scores.unsqueeze(1) + log_probs # [B*K, V]

        # Flatten to find the top K across the whole vocabulary for each batch item
        next_scores = next_scores.view(B, beam_width * vocab_size)
        topk_scores, topk_indices = next_scores.topk(beam_width, dim=1)

        # Map indices back to beam index and token index
        beam_indices = topk_indices // vocab_size  # Which beam did it come from?
        token_indices = topk_indices % vocab_size  # Which character is it?

        # Re-arrange tgt_tokens and beam_scores based on topk
        new_tokens = []
        new_scores = []

        # We need to reconstruct the sequence for each batch item
        for b in range(B):
            batch_offset = b * beam_width
            rows = batch_offset + beam_indices[b]

            # Select the winning beams
            selected_sequences = tgt_tokens[rows]
            selected_tokens = token_indices[b].unsqueeze(1)

            # Concatenate
            tgt_tokens[batch_offset:batch_offset+beam_width] = torch.cat([selected_sequences, selected_tokens], dim=1)
            beam_scores[batch_offset:batch_offset+beam_width] = topk_scores[b]

        # (Optional: Add EOS check here to stop finished beams and move to finished_beams list)
        if i == max_len - 1:
            # At the end, just pick the top beam for each batch
            best_beams = tgt_tokens.view(B, beam_width, -1)[:, 0, :]

    return {"tokens": best_beams}

```

---

### 2. Is Beam Search worth it for OCR?

While it sounds better on paper, here is how to decide whether to use it for your text detection and recognition project:

| Feature | Greedy Decoding | Beam Search |
| --- | --- | --- |
| **Speed** | **Fast** () | **Slower** () |
| **Memory** | Low | High (Multiplied by Beam Width) |
| **Accuracy** | Good for clear text | **Better** for blurry or ambiguous text |
| **Complexity** | Simple | Complex (Harder to `torch.compile`) |

---

### 3. Key Optimization: Temperature & Length Penalty

In Beam Search, longer sequences naturally have lower scores because you keep adding negative log-probabilities.

* **Length Penalty:** Often, we divide the score by  to avoid favoring very short words.
* **Temperature:** If the model is too "confident" but wrong, you can divide logits by a temperature  to smooth the distribution before the Beam Search.

### Next Step

Since this architecture is now capable of full text recognition, would you like me to show you how to implement **Label Smoothing** in your `CrossEntropyLoss`? This is a common trick used with PARSeq to prevent the model from becoming overconfident in its character predictions.


````
