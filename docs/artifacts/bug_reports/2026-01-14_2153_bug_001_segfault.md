---
type: bug_report
category: troubleshooting
status: active
version: 1.0
tags:
  - bug
  - issue
  - troubleshooting
ads_version: 1.0
artifact_type: bug_report
severity: medium
title: DataLoader Segmentation Fault with Multiprocessing
date: 2026-01-14 21:53 (KST)
branch: main
---

# Bug Report - DataLoader Segfault with Multiprocessing
Bug ID: BUG-20260114-001

## Summary
Training job `train.py` crashes with "Unexpected segmentation fault encountered in worker" when using PyTorch DataLoader multiprocessing (`num_workers > 0`) in the recognition domain. This occurs during dataset loading/batching.

## Environment
- **OS/Env**: Linux Container (Docker), /dev/shm size: 8.0G
- **Hardware**: NVIDIA RTX 3090 (24GB VRAM)
- **Dataset**: `recognition/aihub_lmdb_validation` (LMDB format)
- **Dependencies**: PyTorch 2.x, Lightning, Hydra

## Reproduction
Run the training script with `num_workers=4` (or any value > 0):
```bash
uv run python runners/train.py \
    domain=recognition \
    trainer=hardware_rtx3090_24gb_i5_16core \
    exp_name="repro_segfault" \
    +dataloaders.train_dataloader.num_workers=4
```

## Comparisons & Investigation
### Hypotheses Tested
1.  **WandB**: Suspected logging interference. **Disproven**. Crash persists with `wandb=false`.
2.  **LMDB Handle**: Suspected unsafe file handle sharing across forks. **Disproven**. Crash persists even after implementing `__getstate__` to remove environment from pickle.
3.  **Memory/Tensor Size**: Input images are large (~2500x3400). Even though they are resized to `[32, 128]` for the model, the *transfer* from worker to main process via shared memory likely exhausts limits or triggers C-extension instability when handling large raw tensors.

### Resolution
- **Workaround**: Set `num_workers=0` (disable multiprocessing). This is stable and currently applied in `configs/data/dataloaders/default.yaml`.
- **Performance**: Batch formation happens in main process, but GPU utilization remains acceptable for this workload.

## Logs
```
Epoch 0/0  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 22/19262 0:00:02 • 0:29:01 11.05it/s v_num: bdojERROR: Unexpected segmentation fault encountered in worker.
RuntimeError: DataLoader worker (pid(s) 739) exited unexpectedly
```

## Impact
- **Severity**: High (Blocks training if defaults are used)
- **Status**: Mitigated (Config updated to `num_workers: 0`)

