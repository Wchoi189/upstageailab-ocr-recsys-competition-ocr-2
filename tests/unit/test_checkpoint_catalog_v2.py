"""Unit tests for Checkpoint Catalog V2 system.

This module provides comprehensive test coverage for the V2 checkpoint catalog,
including metadata loading, Wandb fallback, caching, and validation.

Test Coverage:
    - Metadata loader (fast path)
    - Wandb client fallback
    - Cache invalidation
    - Validator business rules
    - Catalog builder orchestration
    - Error handling for corrupt metadata
    - Performance regression tests
"""

from __future__ import annotations

import tempfile
from datetime import datetime
from pathlib import Path
from unittest.mock import MagicMock, Mock, patch

import pytest
import yaml

# V2 catalog imports
from ui.apps.inference.services.checkpoint.cache import CatalogCache
from ui.apps.inference.services.checkpoint.catalog import CheckpointCatalogBuilder
from ui.apps.inference.services.checkpoint.metadata_loader import (
    load_metadata,
    load_metadata_batch,
    save_metadata,
)
from ui.apps.inference.services.checkpoint.types import (
    CheckpointingConfig,
    CheckpointMetadataV1,
    DecoderInfo,
    EncoderInfo,
    HeadInfo,
    LossInfo,
    MetricsInfo,
    ModelInfo,
    TrainingInfo,
)
from ui.apps.inference.services.checkpoint.validator import (
    MetadataValidator,
)
from ui.apps.inference.services.checkpoint.wandb_client import (
    WandbClient,
    extract_run_id_from_checkpoint,
)


class TestMetadataLoader:
    """Tests for metadata_loader module (fast path)."""

    @pytest.fixture
    def temp_dir(self):
        """Create temporary directory for test files."""
        with tempfile.TemporaryDirectory() as tmpdir:
            yield Path(tmpdir)

    @pytest.fixture
    def sample_metadata(self):
        """Create sample metadata for testing."""
        return CheckpointMetadataV1(
            schema_version="1.0",
            checkpoint_path="/path/to/checkpoint.ckpt",
            exp_name="test_experiment",
            created_at=datetime.now().isoformat(),
            training=TrainingInfo(
                epoch=10,
                global_step=1000,
                training_phase="training",
                max_epochs=20,
            ),
            model=ModelInfo(
                architecture="dbnet",
                encoder=EncoderInfo(
                    model_name="resnet50",
                    pretrained=True,
                    frozen=False,
                ),
                decoder=DecoderInfo(name="fpn_decoder"),
                head=HeadInfo(name="detection_head"),
                loss=LossInfo(name="db_loss"),
            ),
            metrics=MetricsInfo(
                hmean=0.85,
                precision=0.88,
                recall=0.82,
                validation_loss=0.15,
            ),
            checkpointing=CheckpointingConfig(
                monitor="val/hmean",
                mode="max",
                save_top_k=3,
                save_last=True,
            ),
        )

    def test_save_and_load_metadata(self, temp_dir, sample_metadata):
        """Test saving and loading metadata YAML files."""
        checkpoint_path = temp_dir / "test.ckpt"
        checkpoint_path.touch()

        # Save metadata
        metadata_path = save_metadata(sample_metadata, checkpoint_path)

        # Verify file created
        assert metadata_path.exists()
        assert metadata_path.name == "test.metadata.yaml"

        # Load metadata back
        loaded = load_metadata(checkpoint_path)

        # Verify loaded metadata matches original
        assert loaded is not None
        assert loaded.exp_name == sample_metadata.exp_name
        assert loaded.training.epoch == sample_metadata.training.epoch
        assert loaded.metrics.hmean == sample_metadata.metrics.hmean
        assert loaded.model.architecture == sample_metadata.model.architecture

    def test_load_metadata_missing_file(self, temp_dir):
        """Test loading metadata when file doesn't exist."""
        checkpoint_path = temp_dir / "nonexistent.ckpt"

        metadata = load_metadata(checkpoint_path)

        assert metadata is None

    def test_load_metadata_invalid_yaml(self, temp_dir):
        """Test loading metadata with invalid YAML structure."""
        checkpoint_path = temp_dir / "test.ckpt"
        checkpoint_path.touch()

        metadata_path = checkpoint_path.with_suffix(".metadata.yaml")
        with metadata_path.open("w") as f:
            f.write("not a dict: [1, 2, 3]")

        metadata = load_metadata(checkpoint_path)

        # Should return None for invalid YAML structure
        assert metadata is None

    def test_load_metadata_corrupt_yaml(self, temp_dir):
        """Test loading metadata with corrupt YAML syntax."""
        checkpoint_path = temp_dir / "test.ckpt"
        checkpoint_path.touch()

        metadata_path = checkpoint_path.with_suffix(".metadata.yaml")
        with metadata_path.open("w") as f:
            f.write("{ invalid yaml: [unclosed")

        metadata = load_metadata(checkpoint_path)

        # Should return None for corrupted YAML
        assert metadata is None

    def test_load_metadata_batch(self, temp_dir, sample_metadata):
        """Test batch loading of metadata."""
        # Create multiple checkpoints with metadata
        checkpoint_paths = []
        for i in range(3):
            ckpt_path = temp_dir / f"test_{i}.ckpt"
            ckpt_path.touch()
            sample_metadata.training.epoch = i + 1
            save_metadata(sample_metadata, ckpt_path)
            checkpoint_paths.append(ckpt_path)

        # Batch load
        results = load_metadata_batch(checkpoint_paths)

        # Verify all loaded
        assert len(results) == 3
        for i, ckpt_path in enumerate(checkpoint_paths):
            assert ckpt_path in results
            assert results[ckpt_path] is not None
            assert results[ckpt_path].training.epoch == i + 1

    def test_metadata_excludes_none_values(self, temp_dir):
        """Test that None values are excluded from YAML output."""
        checkpoint_path = temp_dir / "test.ckpt"
        checkpoint_path.touch()

        metadata = CheckpointMetadataV1(
            schema_version="1.0",
            checkpoint_path=str(checkpoint_path),
            exp_name="test",
            created_at=datetime.now().isoformat(),
            training=TrainingInfo(epoch=1, global_step=100, training_phase="training"),
            model=ModelInfo(
                architecture="dbnet",
                encoder=EncoderInfo(model_name="resnet50", pretrained=True, frozen=False),
                decoder=DecoderInfo(name="fpn"),
                head=HeadInfo(name="head"),
                loss=LossInfo(name="loss"),
            ),
            metrics=MetricsInfo(hmean=0.85, precision=None, recall=None),
            checkpointing=CheckpointingConfig(monitor="val/hmean", mode="max"),
        )

        save_metadata(metadata, checkpoint_path)

        # Load raw YAML and check for None values
        with checkpoint_path.with_suffix(".metadata.yaml").open("r") as f:
            raw_data = yaml.safe_load(f)

        # Check that None fields are excluded
        assert "precision" not in raw_data["metrics"]
        assert "recall" not in raw_data["metrics"]


class TestWandbClient:
    """Tests for Wandb API client fallback."""

    def test_wandb_client_initialization_no_api_key(self):
        """Test Wandb client when API key is not set."""
        with patch.dict("os.environ", {}, clear=True):
            client = WandbClient()

            assert not client._is_available
            assert client.api is None

    def test_wandb_client_initialization_with_api_key(self):
        """Test Wandb client initialization with API key."""
        with patch.dict("os.environ", {"WANDB_API_KEY": "test_key"}):
            # wandb is imported conditionally inside the module, so we need to patch it differently
            with patch(
                "builtins.__import__",
                side_effect=lambda name, *args, **kwargs: Mock() if name == "wandb" else __import__(name, *args, **kwargs),
            ):
                client = WandbClient()

                # Should be available
                assert client._is_available

    def test_get_run_config_api_unavailable(self):
        """Test getting run config when API is unavailable."""
        with patch.dict("os.environ", {}, clear=True):
            client = WandbClient()

            config = client.get_run_config("test/project/run_id")

            assert config is None

    @patch("sys.modules", {"wandb": MagicMock()})
    def test_get_run_config_success(self):
        """Test successful run config retrieval."""
        with patch.dict("os.environ", {"WANDB_API_KEY": "test_key"}):
            # Mock wandb module
            mock_run = Mock()
            mock_run.config = {
                "model": {"architecture": "dbnet"},
                "trainer": {"max_epochs": 20},
                "_wandb_internal": "should_be_filtered",
            }

            mock_api = Mock()
            mock_api.run.return_value = mock_run

            # Create client and manually inject mock API
            client = WandbClient()
            client._is_available = True
            client._api = mock_api

            config = client.get_run_config("test/project/run_id")

            # Verify internal keys filtered out
            assert config is not None
            assert "model" in config
            assert "_wandb_internal" not in config

    @patch("sys.modules", {"wandb": MagicMock()})
    def test_get_run_summary_success(self):
        """Test successful run summary retrieval."""
        with patch.dict("os.environ", {"WANDB_API_KEY": "test_key"}):
            # Mock API and run
            mock_run = Mock()
            mock_run.summary = {
                "val/hmean": 0.85,
                "val/precision": 0.88,
                "epoch": 10,
            }

            mock_api = Mock()
            mock_api.run.return_value = mock_run

            # Create client and manually inject mock API
            client = WandbClient()
            client._is_available = True
            client._api = mock_api

            summary = client.get_run_summary("test/project/run_id")

            assert summary is not None
            assert summary["val/hmean"] == 0.85
            assert summary["epoch"] == 10

    def test_extract_run_id_from_metadata_file(self, tmp_path):
        """Test extracting run ID from metadata YAML file."""
        checkpoint_path = tmp_path / "test.ckpt"
        checkpoint_path.touch()

        metadata_path = checkpoint_path.with_suffix(".metadata.yaml")
        with metadata_path.open("w") as f:
            yaml.safe_dump({"wandb_run_id": "test/project/abc123"}, f)

        run_id = extract_run_id_from_checkpoint(checkpoint_path)

        assert run_id == "test/project/abc123"

    def test_extract_run_id_from_hydra_config(self, tmp_path):
        """Test extracting run ID from Hydra config."""
        # Create experiment directory structure
        exp_dir = tmp_path / "experiment"
        exp_dir.mkdir()
        checkpoints_dir = exp_dir / "checkpoints"
        checkpoints_dir.mkdir()

        checkpoint_path = checkpoints_dir / "test.ckpt"
        checkpoint_path.touch()

        # Create Hydra config
        hydra_dir = exp_dir / ".hydra"
        hydra_dir.mkdir()
        config_path = hydra_dir / "config.yaml"

        with config_path.open("w") as f:
            yaml.safe_dump({"logger": {"wandb": {"id": "xyz789"}}}, f)

        run_id = extract_run_id_from_checkpoint(checkpoint_path)

        assert run_id == "xyz789"

    def test_extract_run_id_not_found(self, tmp_path):
        """Test extracting run ID when not available."""
        checkpoint_path = tmp_path / "test.ckpt"
        checkpoint_path.touch()

        run_id = extract_run_id_from_checkpoint(checkpoint_path)

        assert run_id is None


class TestCatalogCache:
    """Tests for catalog caching layer."""

    def test_cache_key_generation(self, tmp_path):
        """Test cache key generation includes path and mtime."""
        cache = CatalogCache()

        key1 = cache.get_cache_key(tmp_path)

        # Key should be deterministic
        key2 = cache.get_cache_key(tmp_path)
        assert key1 == key2

        # Different paths should have different keys
        other_path = tmp_path / "subdir"
        other_path.mkdir()
        key3 = cache.get_cache_key(other_path)
        assert key1 != key3

    def test_cache_get_miss(self, tmp_path):
        """Test cache miss."""
        cache = CatalogCache()

        result = cache.get(tmp_path)

        assert result is None

    def test_cache_set_and_get(self, tmp_path):
        """Test cache set and retrieval."""
        from ui.apps.inference.services.checkpoint.types import CheckpointCatalog

        cache = CatalogCache()
        catalog = CheckpointCatalog(
            entries=[],
            total_count=0,
            metadata_available_count=0,
            catalog_build_time_seconds=0.1,
            outputs_dir=tmp_path,
        )

        cache.set(tmp_path, catalog)
        retrieved = cache.get(tmp_path)

        assert retrieved is not None
        assert retrieved.outputs_dir == tmp_path

    def test_cache_invalidation_on_mtime_change(self, tmp_path):
        """Test that cache invalidates when directory mtime changes."""
        from ui.apps.inference.services.checkpoint.types import CheckpointCatalog

        cache = CatalogCache()
        catalog = CheckpointCatalog(
            entries=[],
            total_count=0,
            metadata_available_count=0,
            catalog_build_time_seconds=0.1,
            outputs_dir=tmp_path,
        )

        # Cache catalog
        cache.set(tmp_path, catalog)

        # Modify directory (change mtime)
        test_file = tmp_path / "newfile.txt"
        test_file.touch()

        # Cache should miss due to mtime change
        cache.get(tmp_path)

        # Note: This test may be flaky on some filesystems
        # where mtime granularity is coarse

    def test_cache_eviction_at_maxsize(self):
        """Test that cache evicts oldest entry when at max size."""
        from ui.apps.inference.services.checkpoint.types import CheckpointCatalog

        cache = CatalogCache(maxsize=2)

        with tempfile.TemporaryDirectory() as tmpdir:
            dir1 = Path(tmpdir) / "dir1"
            dir2 = Path(tmpdir) / "dir2"
            dir3 = Path(tmpdir) / "dir3"

            for d in [dir1, dir2, dir3]:
                d.mkdir()

            catalog1 = CheckpointCatalog(
                entries=[], total_count=1, metadata_available_count=0, catalog_build_time_seconds=0.1, outputs_dir=dir1
            )
            catalog2 = CheckpointCatalog(
                entries=[], total_count=2, metadata_available_count=0, catalog_build_time_seconds=0.1, outputs_dir=dir2
            )
            catalog3 = CheckpointCatalog(
                entries=[], total_count=3, metadata_available_count=0, catalog_build_time_seconds=0.1, outputs_dir=dir3
            )

            # Add first two
            cache.set(dir1, catalog1)
            cache.set(dir2, catalog2)

            # Add third - should evict first
            cache.set(dir3, catalog3)

            # First should be evicted
            assert cache.get(dir1) is None
            # Second and third should remain
            assert cache.get(dir2) is not None
            assert cache.get(dir3) is not None

    def test_cache_clear(self, tmp_path):
        """Test clearing cache."""
        from ui.apps.inference.services.checkpoint.types import CheckpointCatalog

        cache = CatalogCache()
        catalog = CheckpointCatalog(
            entries=[],
            total_count=0,
            metadata_available_count=0,
            catalog_build_time_seconds=0.1,
            outputs_dir=tmp_path,
        )

        cache.set(tmp_path, catalog)
        assert cache.get(tmp_path) is not None

        cache.clear()

        assert cache.get(tmp_path) is None


class TestMetadataValidator:
    """Tests for metadata validation."""

    @pytest.fixture
    def validator(self):
        """Create validator instance."""
        return MetadataValidator()

    @pytest.fixture
    def valid_metadata(self):
        """Create valid metadata for testing."""
        return CheckpointMetadataV1(
            schema_version="1.0",
            checkpoint_path="/path/to/checkpoint.ckpt",
            exp_name="test",
            created_at=datetime.now().isoformat(),
            training=TrainingInfo(epoch=10, global_step=1000, training_phase="training"),
            model=ModelInfo(
                architecture="dbnet",
                encoder=EncoderInfo(model_name="resnet50", pretrained=True, frozen=False),
                decoder=DecoderInfo(name="fpn"),
                head=HeadInfo(name="head"),
                loss=LossInfo(name="loss"),
            ),
            metrics=MetricsInfo(hmean=0.85, precision=0.88, recall=0.82),
            checkpointing=CheckpointingConfig(monitor="val/hmean", mode="max"),
        )

    def test_validate_metadata_success(self, validator, valid_metadata):
        """Test validation of valid metadata."""
        result = validator.validate_metadata(valid_metadata)

        assert result is not None
        assert result.exp_name == "test"

    def test_validate_metadata_missing_hmean(self, validator):
        """Test validation fails when hmean is missing."""
        metadata = CheckpointMetadataV1(
            schema_version="1.0",
            checkpoint_path="/path/to/checkpoint.ckpt",
            exp_name="test",
            created_at=datetime.now().isoformat(),
            training=TrainingInfo(epoch=10, global_step=1000, training_phase="training"),
            model=ModelInfo(
                architecture="dbnet",
                encoder=EncoderInfo(model_name="resnet50", pretrained=True, frozen=False),
                decoder=DecoderInfo(name="fpn"),
                head=HeadInfo(name="head"),
                loss=LossInfo(name="loss"),
            ),
            metrics=MetricsInfo(hmean=None, precision=0.88, recall=0.82),
            checkpointing=CheckpointingConfig(monitor="val/hmean", mode="max"),
        )

        with pytest.raises(ValueError, match="hmean metric is required"):
            validator.validate_metadata(metadata)

    def test_validate_metadata_negative_epoch(self, validator):
        """Test validation fails for negative epoch (caught by Pydantic)."""
        # Pydantic validation should prevent creating metadata with negative epoch
        from pydantic import ValidationError

        with pytest.raises(ValidationError, match="greater than or equal to 0"):
            CheckpointMetadataV1(
                schema_version="1.0",
                checkpoint_path="/path/to/checkpoint.ckpt",
                exp_name="test",
                created_at=datetime.now().isoformat(),
                training=TrainingInfo(epoch=-1, global_step=1000, training_phase="training"),
                model=ModelInfo(
                    architecture="dbnet",
                    encoder=EncoderInfo(model_name="resnet50", pretrained=True, frozen=False),
                    decoder=DecoderInfo(name="fpn"),
                    head=HeadInfo(name="head"),
                    loss=LossInfo(name="loss"),
                ),
                metrics=MetricsInfo(hmean=0.85, precision=0.88, recall=0.82),
                checkpointing=CheckpointingConfig(monitor="val/hmean", mode="max"),
            )

    def test_validate_checkpoint_file_missing(self, validator, tmp_path):
        """Test validation of checkpoint with missing metadata file."""
        checkpoint_path = tmp_path / "test.ckpt"
        checkpoint_path.touch()

        result = validator.validate_checkpoint_file(checkpoint_path)

        assert not result.is_valid
        assert result.error_type == "missing"

    def test_validate_batch(self, validator, valid_metadata):
        """Test batch validation."""
        metadata_list = [valid_metadata]

        results = validator.validate_batch(metadata_list)

        assert len(results) == 1
        assert results[0].exp_name == "test"


class TestCheckpointCatalogBuilder:
    """Tests for checkpoint catalog builder."""

    @pytest.fixture
    def temp_outputs_dir(self):
        """Create temporary outputs directory."""
        with tempfile.TemporaryDirectory() as tmpdir:
            yield Path(tmpdir)

    def test_build_catalog_empty_directory(self, temp_outputs_dir):
        """Test building catalog for empty directory."""
        builder = CheckpointCatalogBuilder(temp_outputs_dir, use_cache=False, use_wandb_fallback=False)

        catalog = builder.build_catalog()

        assert catalog.total_count == 0
        assert catalog.metadata_available_count == 0
        assert len(catalog.entries) == 0

    def test_build_catalog_nonexistent_directory(self):
        """Test building catalog for non-existent directory."""
        builder = CheckpointCatalogBuilder(
            Path("/nonexistent/path"),
            use_cache=False,
            use_wandb_fallback=False,
        )

        catalog = builder.build_catalog()

        assert catalog.total_count == 0

    def test_build_catalog_with_metadata_files(self, temp_outputs_dir):
        """Test building catalog with metadata YAML files (fast path)."""
        # Create experiment structure
        exp_dir = temp_outputs_dir / "experiment_001"
        exp_dir.mkdir()
        ckpt_dir = exp_dir / "checkpoints"
        ckpt_dir.mkdir()

        # Create checkpoint with metadata
        ckpt_path = ckpt_dir / "epoch=10.ckpt"
        ckpt_path.touch()

        metadata = CheckpointMetadataV1(
            schema_version="1.0",
            checkpoint_path=str(ckpt_path),
            exp_name="experiment_001",
            created_at=datetime.now().isoformat(),
            training=TrainingInfo(epoch=10, global_step=1000, training_phase="training"),
            model=ModelInfo(
                architecture="dbnet",
                encoder=EncoderInfo(model_name="resnet50", pretrained=True, frozen=False),
                decoder=DecoderInfo(name="fpn_decoder"),
                head=HeadInfo(name="detection_head"),
                loss=LossInfo(name="db_loss"),
            ),
            metrics=MetricsInfo(hmean=0.85, precision=0.88, recall=0.82),
            checkpointing=CheckpointingConfig(monitor="val/hmean", mode="max"),
        )

        save_metadata(metadata, ckpt_path)

        # Build catalog
        builder = CheckpointCatalogBuilder(temp_outputs_dir, use_cache=False, use_wandb_fallback=False)
        catalog = builder.build_catalog()

        # Verify catalog
        assert catalog.total_count == 1
        assert catalog.metadata_available_count == 1
        assert len(catalog.entries) == 1

        entry = catalog.entries[0]
        assert entry.checkpoint_path == ckpt_path
        assert entry.architecture == "dbnet"
        assert entry.backbone == "resnet50"
        assert entry.epochs == 10
        assert entry.hmean == 0.85
        assert entry.has_metadata is True

    def test_build_catalog_with_caching(self, temp_outputs_dir):
        """Test that catalog caching works correctly."""
        # Create checkpoint with metadata
        exp_dir = temp_outputs_dir / "experiment"
        exp_dir.mkdir()
        ckpt_dir = exp_dir / "checkpoints"
        ckpt_dir.mkdir()

        ckpt_path = ckpt_dir / "test.ckpt"
        ckpt_path.touch()

        metadata = CheckpointMetadataV1(
            schema_version="1.0",
            checkpoint_path=str(ckpt_path),
            exp_name="experiment",
            created_at=datetime.now().isoformat(),
            training=TrainingInfo(epoch=10, global_step=1000, training_phase="training"),
            model=ModelInfo(
                architecture="dbnet",
                encoder=EncoderInfo(model_name="resnet50", pretrained=True, frozen=False),
                decoder=DecoderInfo(name="fpn"),
                head=HeadInfo(name="head"),
                loss=LossInfo(name="loss"),
            ),
            metrics=MetricsInfo(hmean=0.85, precision=0.88, recall=0.82),
            checkpointing=CheckpointingConfig(monitor="val/hmean", mode="max"),
        )

        save_metadata(metadata, ckpt_path)

        # Build catalog with caching enabled
        builder = CheckpointCatalogBuilder(temp_outputs_dir, use_cache=True, use_wandb_fallback=False)

        # First build - should populate cache
        catalog1 = builder.build_catalog()

        # Second build - should hit cache (returns same instance)
        catalog2 = builder.build_catalog()

        # Cached build should be identical (same object returned from cache)
        assert catalog2.total_count == catalog1.total_count
        assert catalog2.metadata_available_count == catalog1.metadata_available_count
        # Verify cache was actually used (catalog_build_time_seconds will be from first build)
        assert catalog2 is catalog1 or catalog2.catalog_build_time_seconds == catalog1.catalog_build_time_seconds

    def test_extract_epoch_from_filename(self):
        """Test epoch extraction from various filename patterns."""
        builder = CheckpointCatalogBuilder(Path("/tmp"), use_cache=False, use_wandb_fallback=False)

        # Test various patterns
        assert builder._extract_epoch_from_filename("epoch=10") == 10
        assert builder._extract_epoch_from_filename("epoch-5") == 5
        assert builder._extract_epoch_from_filename("epoch_15") == 15
        assert builder._extract_epoch_from_filename("best") == 999
        assert builder._extract_epoch_from_filename("last") == 998
        assert builder._extract_epoch_from_filename("no_epoch_here") is None

    def test_maybe_float_conversion(self):
        """Test float conversion utility."""
        builder = CheckpointCatalogBuilder(Path("/tmp"), use_cache=False, use_wandb_fallback=False)

        assert builder._maybe_float(0.85) == 0.85
        assert builder._maybe_float(42) == 42.0
        assert builder._maybe_float("0.95") == 0.95
        assert builder._maybe_float(" 0.75 ") == 0.75
        assert builder._maybe_float(None) is None
        assert builder._maybe_float("invalid") is None


class TestPerformanceRegression:
    """Performance regression tests for V2 catalog system."""

    def test_metadata_loading_performance(self, tmp_path):
        """Test that metadata loading is fast (<50ms per checkpoint)."""
        import time

        # Create checkpoint with metadata
        ckpt_path = tmp_path / "test.ckpt"
        ckpt_path.touch()

        metadata = CheckpointMetadataV1(
            schema_version="1.0",
            checkpoint_path=str(ckpt_path),
            exp_name="perf_test",
            created_at=datetime.now().isoformat(),
            training=TrainingInfo(epoch=10, global_step=1000, training_phase="training"),
            model=ModelInfo(
                architecture="dbnet",
                encoder=EncoderInfo(model_name="resnet50", pretrained=True, frozen=False),
                decoder=DecoderInfo(name="fpn"),
                head=HeadInfo(name="head"),
                loss=LossInfo(name="loss"),
            ),
            metrics=MetricsInfo(hmean=0.85, precision=0.88, recall=0.82),
            checkpointing=CheckpointingConfig(monitor="val/hmean", mode="max"),
        )

        save_metadata(metadata, ckpt_path)

        # Measure loading time
        start = time.perf_counter()
        for _ in range(10):
            load_metadata(ckpt_path)
        elapsed = time.perf_counter() - start

        # Should be <50ms per load on average
        avg_time_ms = (elapsed / 10) * 1000
        assert avg_time_ms < 50, f"Metadata loading too slow: {avg_time_ms:.2f}ms (target: <50ms)"

    def test_catalog_build_performance(self, tmp_path):
        """Test that catalog building is reasonably fast with metadata."""
        # Create multiple checkpoints
        exp_dir = tmp_path / "experiment"
        exp_dir.mkdir()
        ckpt_dir = exp_dir / "checkpoints"
        ckpt_dir.mkdir()

        num_checkpoints = 10

        for i in range(num_checkpoints):
            ckpt_path = ckpt_dir / f"epoch={i}.ckpt"
            ckpt_path.touch()

            metadata = CheckpointMetadataV1(
                schema_version="1.0",
                checkpoint_path=str(ckpt_path),
                exp_name="experiment",
                created_at=datetime.now().isoformat(),
                training=TrainingInfo(epoch=i, global_step=i * 100, training_phase="training"),
                model=ModelInfo(
                    architecture="dbnet",
                    encoder=EncoderInfo(model_name="resnet50", pretrained=True, frozen=False),
                    decoder=DecoderInfo(name="fpn"),
                    head=HeadInfo(name="head"),
                    loss=LossInfo(name="loss"),
                ),
                metrics=MetricsInfo(hmean=0.85, precision=0.88, recall=0.82),
                checkpointing=CheckpointingConfig(monitor="val/hmean", mode="max"),
            )

            save_metadata(metadata, ckpt_path)

        # Build catalog
        builder = CheckpointCatalogBuilder(tmp_path, use_cache=False, use_wandb_fallback=False)
        catalog = builder.build_catalog()

        # Should complete in <1 second for 10 checkpoints
        assert catalog.catalog_build_time_seconds < 1.0, (
            f"Catalog build too slow: {catalog.catalog_build_time_seconds:.3f}s (target: <1.0s for {num_checkpoints} checkpoints)"
        )


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
