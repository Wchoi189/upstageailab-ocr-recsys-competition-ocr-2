---
ads_version: "1.0"
type: rule_set
agent: all
tier: 2
priority: high
validates_with: "AgentQMS/standards/schemas/compliance-checker.py"
compliance_status: unknown
memory_footprint: 120
date: "2025-12-24"
status: active
depends_on: []

# Tier 2 - Coding Standards: Import Optimization

heavy_modules:
  ml_frameworks:
    - torch
    - torchvision
    - lightning
    - tensorflow
    - jax
  data_processing:
    - numpy  # Only in test contexts
    - pandas
    - scipy
  model_libraries:
    - timm
    - transformers
    - huggingface_hub

lazy_import_rules:
  scope: "test files, fixtures, optional feature modules"

  must_lazy_import:
    - "All heavy_modules in test_*.py files"
    - "All heavy_modules in conftest.py fixtures"
    - "Optional dependencies in library code"

  allowed_module_level:
    - "pytest"
    - "typing"
    - "pathlib"
    - "dataclasses"
    - "Standard library lightweight modules"

  patterns:
    prohibited:
      location: "module level in test files"
      example: |
        import torch
        import numpy as np
        def test_foo(): ...

    required:
      location: "inside function/fixture body"
      example: |
        def test_foo():
            import torch
            import numpy as np
            ...

        @pytest.fixture
        def my_fixture():
            import torch
            return torch.randn(3, 224, 224)

test_file_rules:
  conftest_py:
    - "NO module-level torch/numpy imports"
    - "Import inside each fixture body"
    - "Use function-scoped fixtures when importing heavy modules"

  test_modules:
    - "Import heavy dependencies inside test functions"
    - "Use TYPE_CHECKING guards for type hints only"
    - "Group lazy imports at function start"

rationale_for_ai:
  objective: "Reduce pytest collection time"
  impact: "Heavy imports at module level execute during collection"
  target: "Collection time < execution overhead of lazy imports"

enforcement:
  detection: "grep for module-level imports in test files"
  future_hook: "pre-commit check for test file imports"
  command: |
    grep -rn "^import torch\|^import numpy\|^from torch\|^from numpy" tests/

# Tier 2 - Coding Standards: Pydantic Data Validation

pydantic_validation_rules:
  version: "pydantic v2"

  mandatory_locations:
    - "Dataset __getitem__ inputs/outputs"
    - "Transform pipeline input/output contracts"
    - "API request/response models"
    - "Configuration dataclasses"
    - "Cache serialization boundaries"

  prohibited_patterns:
    - "Passing raw dicts between pipeline stages"
    - "Unvalidated numpy array shapes"
    - "Tensor dtype assumptions without validation"
    - "Config values accessed without schema"

  required_features:
    config: "ConfigDict(arbitrary_types_allowed=True)"
    field_validation: "field_validator for shape/dtype checks"
    cross_field: "model_validator for cross-field validation"
    errors: "Descriptive ValidationError messages"

  patterns:
    prohibited:
      description: "Raw dict passing"
      example: |
        def process(data: dict) -> dict:
            return {"result": data["input"] * 2}

    required:
      description: "Pydantic model contracts"
      example: |
        class ProcessInput(BaseModel):
            model_config = ConfigDict(arbitrary_types_allowed=True)
            input: np.ndarray

            @field_validator("input")
            @classmethod
            def validate_shape(cls, v):
                if v.ndim != 2:
                    raise ValueError("Must be 2D")
                return v

        class ProcessOutput(BaseModel):
            result: np.ndarray

        def process(data: ProcessInput) -> ProcessOutput:
            return ProcessOutput(result=data.input * 2)

  reference_implementations:
    core_validation: "ocr/core/validation.py"  # ← Primary (all models/schemas)
    schemas_shim: "ocr/data/datasets/schemas.py"  # ← Recognition specific
    validation_shim: "ocr/core/validation.py"  # ← Consolidated
    contracts: "ocr/data/datasets/preprocessing/contracts.py" # (verify path)
    transforms: "ocr/data/datasets/transforms.py"

# Tier 2 - Coding Standards: Error Handling

error_handling_rules:
  exception_hierarchy:
    base: "Use domain-specific exception classes"
    locations:
      - "ocr/core/exceptions.py if exists, else standard Python exceptions"
      - "Inherit from built-in exceptions (ValueError, TypeError)"

  required_patterns:
    - "Always include context in error messages"
    - "Use 'from exc' for exception chaining"
    - "Log before re-raising in boundaries"

  prohibited_patterns:
    - "Bare except clauses"
    - "Catching Exception without re-raise"
    - "Silent failures (empty except blocks)"
    - "String-only error messages without context"

  patterns:
    prohibited:
      example: |
        try:
            process(data)
        except:
            pass

    required:
      example: |
        try:
            process(data)
        except ValidationError as exc:
            logger.error("Processing failed for %s: %s", filename, exc)
            raise ProcessingError(f"Failed to process {filename}") from exc

  retry_logic:
    when: "Network calls, transient failures only"
    library: "tenacity"
    max_attempts: 3
    backoff: "exponential"

# Tier 2 - Coding Standards: Logging

logging_rules:
  levels:
    DEBUG: "Verbose diagnostic info (disabled in prod)"
    INFO: "Key operations, state changes"
    WARNING: "Recoverable issues, degraded performance"
    ERROR: "Failures requiring attention"
    CRITICAL: "System-wide failures"

  required_patterns:
    - "Use module-level logger: logger = logging.getLogger(__name__)"
    - "Use %-formatting for lazy evaluation: logger.info('x=%s', x)"
    - "Include relevant context (filename, batch_idx, etc.)"

  prohibited_patterns:
    - "print() for logging"
    - "f-strings in log calls (eager evaluation)"
    - "Logging secrets, passwords, API keys"
    - "Logging entire tensors or large arrays"

  patterns:
    prohibited:
      example: |
        print(f"Processing {filename}")
        logger.info(f"Result: {large_tensor}")

    required:
      example: |
        logger = logging.getLogger(__name__)
        logger.info("Processing file: %s", filename)
        logger.debug("Tensor shape: %s", tensor.shape)

# Tier 2 - Coding Standards: Type Annotations

type_annotation_rules:
  required_locations:
    - "All public function signatures"
    - "All class attributes"
    - "All module-level constants"

  prohibited_patterns:
    - "Using Any without justification"
    - "Missing return type annotations"
    - "Untyped **kwargs in public APIs"

  recommended_patterns:
    type_checking_guard:
      purpose: "Avoid import overhead for type hints"
      example: |
        from typing import TYPE_CHECKING
        if TYPE_CHECKING:
            import torch
            from ocr.models import DBHead

        def process(x: "torch.Tensor") -> "DBHead":
            import torch
            ...

    optional_types:
      pattern: "X | None instead of Optional[X]"
      example: "def process(data: np.ndarray | None = None)"

    collection_types:
      pattern: "list[X] instead of List[X]"
      example: "def process(items: list[str]) -> dict[str, int]"

  enforcement:
    tool: "mypy --strict"
    config: "pyproject.toml"

# Tier 2 - Coding Standards: Asynchronous Patterns

async_patterns_rules:
  fastapi_routes:
    blocking_operations:
      description: "CPU-bound tasks (inference, heavy image proc) in async def block the event loop"
      rule: "Use 'def' (threadpool) OR 'await loop.run_in_executor' for blocking calls"

      patterns:
        prohibited:
          description: "Blocking call in async def"
          example: |
            @app.post("/predict")
            async def predict(image):
                # BLOCKS the event loop! No other requests processed.
                return model.forward(image)

        recommended:
          description: "Starlette ThreadPool (Simplest)"
          example: |
            @app.post("/predict")
            def predict(image):
                # FastAPI runs this in a threadpool automatically
                return model.forward(image)

        advanced:
          description: "Explicit Executor (Best for mixed workloads)"
          example: |
            @app.post("/predict")
            async def predict(image):
                # Non-blocking preload
                await check_cache()
                # Run blocking inference in specific executor
                loop = asyncio.get_running_loop()
                return await loop.run_in_executor(model_executor, model.forward, image)

  io_operations:
    network_calls: "MUST use async clients (httpx, aiohttp) instead of sync (requests)"
    file_io: "Prefer aiofiles for large files, or run_in_executor for pathlib ops"
    database: "Use async drivers (asyncpg, motor) where possible"

# Tier 2 - Coding Standards: Path Resolution

path_resolution_rules:
  objective: "Ensure robust path resolution independent of file location"

  required_patterns:
    - "Use ocr.core.utils.path_utils.PROJECT_ROOT for root-relative paths"
    - "Use simple / operator for path joining"

  prohibited_patterns:
    - "Chained .parent calls (more than 2) to find root"
    - "Hardcoded relative paths assuming directory depth"

  patterns:
    prohibited:
      description: "Fragile parent traversal"
      example: |
        # BAD: Breaks if file is moved
        ROOT = Path(__file__).resolve().parent.parent.parent

    required:
      description: "Robust root import"
      example: |
        # GOOD: Works anywhere
        from ocr.core.utils.path_utils import PROJECT_ROOT
        data_file = PROJECT_ROOT / "data" / "file.txt"

  enforcement:
    detection: "grep for excessive parent chaining"
    command: |
      grep -rn "\.parent\.parent\.parent" .

# Tier 2 - Coding Standards: Deprecation Protocol

deprecation_rules:
  lifecycle_phases:
    phase_1_soft:
      action: "Add @deprecated decorator or DeprecationWarning"
      timeline: "Immediate upon decision"
      behavior: "Code works, logs warning once per session"

    phase_2_hard:
      action: "Make warning loud (stack trace) or soft-fail"
      timeline: "Next minor version (e.g. v1.1 -> v1.2)"
      behavior: "Code works but spams warnings or requires flag to enable"

    phase_3_removal:
      action: "Delete code"
      timeline: "Next major version (e.g. v1.x -> v2.0)"
      behavior: "ImportError or AttributeError"

  required_patterns:
    python_code:
      description: "Use the warnings library"
      example: |
        import warnings

        def old_function():
            warnings.warn(
                "old_function is deprecated and will be removed in v2.0. Use new_function instead.",
                DeprecationWarning,
                stacklevel=2
            )
            return new_function()

    api_endpoints:
      description: "Mark in FastAPI/OpenAPI"
      example: |
        @app.get("/old-route", deprecated=True)
        def old_route(): ...

    config_flags:
      description: "Mark deprecated config fields"
      example: "Add (Deprecated) prefix to help text or log warning in validator"

  documentation:
    requirement: "Must update changelog and migration guide immediately"
