#!/usr/bin/env python3
"""
Pipeline Contract Validation Script

Validates that all OCR pipeline components maintain data contracts.
Run this before committing changes to prevent contract violations.

Usage:
    python scripts/analysis_validation/validate_pipeline_contracts.py
    python scripts/analysis_validation/validate_pipeline_contracts.py --verbose
    python scripts/analysis_validation/validate_pipeline_contracts.py --component dataset
"""

import argparse
import sys
from pathlib import Path
from typing import Any

import numpy as np
import torch
from PIL import Image

# Ensure repository root is importable when running the script directly
REPO_ROOT = Path(__file__).resolve().parents[2]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

import albumentations as A

from ocr.datasets import DBCollateFN, ValidatedOCRDataset
from ocr.datasets.schemas import DatasetConfig
from ocr.datasets.transforms import DBTransforms


class ContractValidator:
    """Validates data contracts across OCR pipeline components."""

    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.errors: list[str] = []
        self.warnings: list[str] = []

    def log(self, message: str) -> None:
        """Log message if verbose mode."""
        if self.verbose:
            print(message)

    def error(self, message: str) -> None:
        """Record an error."""
        self.errors.append(message)
        print(f"‚ùå ERROR: {message}")

    def warning(self, message: str) -> None:
        """Record a warning."""
        self.warnings.append(message)
        print(f"‚ö†Ô∏è  WARNING: {message}")

    def success(self, message: str) -> None:
        """Log success message."""
        self.log(f"‚úÖ {message}")

    def validate_dataset_contract(self, sample: dict[str, Any]) -> bool:
        """Validate ValidatedOCRDataset output contract."""
        self.log("Validating dataset contract...")

        # Required keys (maps are generated by collate function, not dataset)
        required_keys = ["image", "polygons", "metadata", "inverse_matrix"]

        for key in required_keys:
            if key not in sample:
                self.error(f"Missing required key: {key}")
                return False

        # Image validation (can be numpy array or torch tensor after transforms)
        image = sample["image"]
        if not isinstance(image, np.ndarray | torch.Tensor):
            self.error(f"Image must be numpy array or torch tensor, got {type(image)}")
            return False

        if isinstance(image, torch.Tensor):
            # For torch tensors, shape is (C, H, W)
            if image.ndim != 3:
                self.error(f"Image tensor must be 3D (C, H, W), got {image.ndim}D")
                return False
            if image.shape[0] != 3:
                self.error(f"Image tensor must have 3 channels, got {image.shape[0]}")
                return False
        else:
            # For numpy arrays, shape is (H, W, C)
            if image.ndim != 3:
                self.error(f"Image array must be 3D (H, W, C), got {image.ndim}D")
                return False
            if image.shape[2] != 3:
                self.error(f"Image array must have 3 channels, got {image.shape[2]}")
                return False

        # Polygon validation
        polygons = sample["polygons"]
        if not isinstance(polygons, list):
            self.error(f"Polygons must be list, got {type(polygons)}")
            return False

        for i, polygon in enumerate(polygons):
            if not isinstance(polygon, np.ndarray):
                self.error(f"Polygon {i} must be numpy array, got {type(polygon)}")
                return False

            # Accept both 2D (N, 2) and 3D (1, N, 2) shapes from transforms
            if polygon.ndim == 2:
                # Shape should be (N, 2)
                if polygon.shape[1] != 2:
                    self.error(f"Polygon {i} must have 2 coordinates, got {polygon.shape[1]}")
                    return False
                num_points = polygon.shape[0]
            elif polygon.ndim == 3:
                # Shape should be (1, N, 2) for batched polygons
                if polygon.shape[0] != 1 or polygon.shape[2] != 2:
                    self.error(f"Polygon {i} 3D shape must be (1, N, 2), got {polygon.shape}")
                    return False
                num_points = polygon.shape[1]
            else:
                self.error(f"Polygon {i} must be 2D or 3D, got {polygon.ndim}D")
                return False

            if num_points < 3:
                self.warning(f"Polygon {i} has only {num_points} points (minimum 3)")

        # Metadata validation (new dataset contract stores filename/path here)
        metadata = self._extract_metadata(sample)
        if not metadata:
            self.error("Metadata is required but missing or invalid")
            return False

        metadata_required = ["filename", "path", "original_shape"]
        for key in metadata_required:
            if key not in metadata or metadata[key] is None:
                self.error(f"Metadata missing required field: {key}")
                return False

        if not isinstance(metadata["filename"], str):
            self.error(f"Metadata filename must be str, got {type(metadata['filename'])}")
            return False

        if not isinstance(metadata["path"], str | Path):
            self.error(f"Metadata path must be str or Path, got {type(metadata['path'])}")
            return False

        original_shape = metadata["original_shape"]
        if not (isinstance(original_shape, tuple) and len(original_shape) == 2):
            self.error(f"Metadata original_shape must be tuple(height, width), got {original_shape}")
            return False

        raw_size = metadata.get("raw_size")
        if raw_size is not None:
            if not (isinstance(raw_size, tuple) and len(raw_size) == 2):
                self.error(f"Metadata raw_size must be tuple(width, height), got {raw_size}")
                return False

        orientation = metadata.get("orientation")
        if orientation is not None and not isinstance(orientation, int):
            self.error(f"Metadata orientation must be int, got {type(orientation)}")
            return False

        # Optional map validation (maps are generated by collate function if not present)
        for map_name in ["prob_map", "thresh_map"]:
            map_data = sample.get(map_name)
            if map_data is None:
                continue

            if not isinstance(map_data, np.ndarray):
                self.error(f"{map_name} must be numpy array, got {type(map_data)}")
                return False

            if map_data.ndim not in (2, 3):
                self.error(f"{map_name} must be 2D or 3D, got {map_data.ndim}D")
                return False

            if map_data.ndim == 3:
                if map_data.shape[0] != 1:
                    self.error(f"{map_name} first dimension should be channel=1, got {map_data.shape[0]}")
                    return False
                map_height, map_width = map_data.shape[1], map_data.shape[2]
            else:
                map_height, map_width = map_data.shape

            # Get image dimensions based on type
            if isinstance(image, torch.Tensor):
                # Torch tensor shape is (C, H, W)
                image_height, image_width = image.shape[1], image.shape[2]
            else:
                # Numpy array shape is (H, W, C)
                image_height, image_width = image.shape[0], image.shape[1]

            if (map_height, map_width) != (image_height, image_width):
                self.error(f"{map_name} spatial shape ({map_height}, {map_width}) != image shape ({image_height}, {image_width})")
                return False

            if not np.all(np.isfinite(map_data)):
                self.warning(f"{map_name} contains non-finite values")

        self.success("Dataset contract validation passed")
        return True

    def validate_transform_contract(self, input_data: dict[str, Any], output_data: dict[str, Any]) -> bool:
        """Validate DBTransforms contract."""
        self.log("Validating transform contract...")

        # Input validation
        if "image" not in input_data or "polygons" not in input_data:
            self.error("Transform input missing required keys")
            return False

        # Output validation
        required_output_keys = ["image", "polygons"]
        for key in required_output_keys:
            if key not in output_data:
                self.error(f"Transform output missing required key: {key}")
                return False

        # Output image should be torch tensor
        output_image = output_data["image"]
        if not isinstance(output_image, torch.Tensor):
            self.error(f"Transform output image must be torch tensor, got {type(output_image)}")
            return False

        if output_image.ndim != 3:
            self.error(f"Transform output image must be 3D (C, H, W), got {output_image.ndim}D")
            return False

        if output_image.shape[0] != 3:
            self.error(f"Transform output image must have 3 channels, got {output_image.shape[0]}")
            return False

        self.success("Transform contract validation passed")
        return True

    def validate_collate_contract(self, batch: dict[str, Any]) -> bool:
        """Validate DBCollateFN contract."""
        self.log("Validating collate contract...")

        # Required keys
        required_keys = [
            "images",
            "polygons",
            "prob_maps",
            "thresh_maps",
            "image_filename",
            "image_path",
            "inverse_matrix",
            "raw_size",
            "orientation",
            "canonical_size",
        ]

        for key in required_keys:
            if key not in batch:
                self.error(f"Collate output missing required key: {key}")
                return False

        # Batch tensors should have consistent batch dimension
        batch_size = batch["images"].shape[0]

        for tensor_name in ["images", "prob_maps", "thresh_maps"]:
            tensor = batch[tensor_name]
            if not isinstance(tensor, torch.Tensor):
                self.error(f"{tensor_name} must be torch tensor, got {type(tensor)}")
                return False

            if tensor.shape[0] != batch_size:
                self.error(f"{tensor_name} batch size {tensor.shape[0]} != expected {batch_size}")
                return False

        # Single fields (not batched)
        single_fields = ["image_filename", "image_path", "inverse_matrix", "raw_size", "orientation", "canonical_size"]
        for field_name in single_fields:
            if not isinstance(batch[field_name], list):
                self.error(f"{field_name} should be a list for batching, got {type(batch[field_name])}")
                return False
            if len(batch[field_name]) != batch_size:
                self.error(f"{field_name} length {len(batch[field_name])} != batch size {batch_size}")
                return False

        # Polygons field should match batch size
        if len(batch["polygons"]) != batch_size:
            self.error(f"polygons length {len(batch['polygons'])} != batch size {batch_size}")
            return False

        # Polygon validation
        for i, polygons in enumerate(batch["polygons"]):
            if not isinstance(polygons, list):
                self.error(f"Sample {i} polygons must be list, got {type(polygons)}")
                return False

            for j, polygon in enumerate(polygons):
                if not isinstance(polygon, np.ndarray):
                    self.error(f"Sample {i} polygon {j} must be numpy array, got {type(polygon)}")
                    return False

                if polygon.ndim != 2 or polygon.shape[1] != 2:
                    self.error(f"Sample {i} polygon {j} must be (N, 2), got {polygon.shape}")
                    return False

        self.success("Collate contract validation passed")
        return True

    def _extract_metadata(self, sample: dict[str, Any]) -> dict[str, Any]:
        """Normalize metadata payloads regardless of originating type."""
        metadata_obj = sample.get("metadata")
        if metadata_obj is None:
            return {}
        if hasattr(metadata_obj, "model_dump"):
            return metadata_obj.model_dump()
        if isinstance(metadata_obj, dict):
            return metadata_obj
        return {}

    def _prepare_for_collate(self, sample: dict[str, Any]) -> dict[str, Any]:
        """Remove empty map placeholders so collate can regenerate them."""
        prepared = sample.copy()
        if prepared.get("prob_map") is None:
            prepared.pop("prob_map", None)
        if prepared.get("thresh_map") is None:
            prepared.pop("thresh_map", None)
        return prepared

    def create_test_data(self) -> tuple[ValidatedOCRDataset, DBTransforms, DBCollateFN]:
        """Create test dataset and components for validation."""
        # Create minimal test data
        test_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
        test_polygons = [np.array([[50, 50], [150, 50], [150, 150], [50, 150]], dtype=np.float32)]
        np.random.rand(224, 224).astype(np.float32)
        np.random.rand(224, 224).astype(np.float32)

        # Create temporary directory and files
        import json
        import tempfile

        temp_dir = tempfile.mkdtemp()
        img_filename = "test_image.jpg"
        img_path = Path(temp_dir) / img_filename

        # Create temporary image file
        Image.fromarray(test_image).save(img_path)

        # Create temporary annotation file
        annotations = {"images": {img_filename: {"words": {"word1": {"points": test_polygons[0].tolist()}}}}}

        anno_path = Path(temp_dir) / "annotations.json"
        with open(anno_path, "w") as f:
            json.dump(annotations, f)

        # Create components
        transforms = DBTransforms(
            transforms=[A.Resize(224, 224)],
            keypoint_params=A.KeypointParams(format="xy", remove_invisible=False),
        )

        dataset_config = DatasetConfig(
            image_path=Path(temp_dir),
            annotation_path=Path(anno_path),
            preload_images=False,
            preload_maps=False,
            load_maps=False,
        )

        dataset = ValidatedOCRDataset(config=dataset_config, transform=transforms)

        collate_fn = DBCollateFN()

        # Cleanup temp directory will happen automatically
        import atexit
        import shutil

        atexit.register(lambda: shutil.rmtree(temp_dir, ignore_errors=True))

        return dataset, transforms, collate_fn

    def validate_full_pipeline(self) -> bool:
        """Validate complete pipeline contracts."""
        self.log("üîç Starting full pipeline contract validation...")

        try:
            # Create test components
            dataset, transforms, collate_fn = self.create_test_data()

            # Test dataset contract
            sample = dataset[0]
            if not self.validate_dataset_contract(sample):
                return False

            # Test transform contract (transforms are applied in dataset)
            # Note: transforms are applied during dataset.__getitem__

            # Test collate contract
            collate_ready = self._prepare_for_collate(sample)
            batch = collate_fn([collate_ready])
            if not self.validate_collate_contract(batch):
                return False

            self.success("Full pipeline contract validation PASSED")
            return True

        except Exception as e:
            self.error(f"Pipeline validation failed with exception: {e}")
            if self.verbose:
                import traceback

                traceback.print_exc()
            return False

    def validate_component(self, component: str) -> bool:
        """Validate specific component."""
        if component == "dataset":
            dataset, _, _ = self.create_test_data()
            sample = dataset[0]
            return self.validate_dataset_contract(sample)
        elif component == "collate":
            dataset, _, collate_fn = self.create_test_data()
            sample = dataset[0]
            collate_ready = self._prepare_for_collate(sample)
            batch = collate_fn([collate_ready])
            return self.validate_collate_contract(batch)
        else:
            self.error(f"Unknown component: {component}")
            return False

    def report(self) -> bool:
        """Generate validation report."""
        print("\n" + "=" * 60)
        print("üìã CONTRACT VALIDATION REPORT")
        print("=" * 60)

        if not self.errors and not self.warnings:
            print("‚úÖ ALL CONTRACTS VALID - No errors or warnings")
            return True

        if self.errors:
            print(f"‚ùå {len(self.errors)} ERRORS found:")
            for error in self.errors:
                print(f"   ‚Ä¢ {error}")
            print()

        if self.warnings:
            print(f"‚ö†Ô∏è  {len(self.warnings)} WARNINGS found:")
            for warning in self.warnings:
                print(f"   ‚Ä¢ {warning}")
            print()

        print("üîó See docs/pipeline/data_contracts.md for contract specifications")
        print("üîß See docs/troubleshooting/shape_issues.md for fixing guidance")

        return len(self.errors) == 0


def main():
    parser = argparse.ArgumentParser(description="Validate OCR pipeline data contracts")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    parser.add_argument("--component", "-c", choices=["dataset", "collate", "full"], default="full", help="Component to validate")

    args = parser.parse_args()

    validator = ContractValidator(verbose=args.verbose)

    if args.component == "full":
        success = validator.validate_full_pipeline()
    else:
        success = validator.validate_component(args.component)

    validator.report()

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
