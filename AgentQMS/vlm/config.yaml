# Centralized configuration for VLM tools

backends:
  # Default to API backend; CLI backend is opt-in and requires a compatible
  # qwen-vl style binary configured via QWEN_VLM_COMMAND.
  default: "openrouter"
  priority:
    - "openrouter"
    - "solar_pro2"
    - "cli"

  openrouter:
    base_url: "https://openrouter.ai/api/v1"
    default_model: "google/gemma-3-27b-it:free"
    api_key_env: "GEMMA27B"

  solar_pro2:
    endpoint: "https://api.upstage.ai/v1"
    default_model: "solar-pro2"
    api_key_env: "SOLAR_PRO2_API_KEY"

  cli:
    # NOTE: The generic `qwen` Code CLI does NOT support the VLM interface
    # expected by CLIQwenBackend (analyze --image --prompt-file --mode).
    # To enable the cli backend, set QWEN_VLM_COMMAND to a compatible binary
    # (for example, a qwen-vl wrapper) inside the container.
    default_command: "qwen"
    command_env: "QWEN_VLM_COMMAND"
    model: "qwen3-vl-plus-2025-09-23"
    # Optional extra CLI flags, e.g. forcing JSON output:
    # extra_args:
    #   - "--output-format"
    #   - "json"

image:
  max_resolution: 2048
  max_dimension: 8192
  default_quality: 95
  supported_formats:
    - "JPEG"
    - "PNG"
    - "WEBP"

backend_defaults:
  timeout_seconds: 60
  max_retries: 3
  max_resolution: 2048

env:
  search_paths:
    - "AgentQMS/vlm"
    - "."
  files:
    - ".env"
    - ".env.local"
  priority: "local"  # .env.local overrides .env
