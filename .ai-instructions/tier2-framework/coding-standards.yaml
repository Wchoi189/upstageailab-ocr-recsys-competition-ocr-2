---
ads_version: "1.0"
type: rule_set
agent: all
tier: 2
priority: high
validates_with: ".ai-instructions/schema/compliance-checker.py"
compliance_status: unknown
memory_footprint: 120
date: "2025-12-23"
status: active
depends_on: []

# Tier 2 - Coding Standards: Import Optimization

heavy_modules:
  ml_frameworks:
    - torch
    - torchvision
    - lightning
    - tensorflow
    - jax
  data_processing:
    - numpy  # Only in test contexts
    - pandas
    - scipy
  model_libraries:
    - timm
    - transformers
    - huggingface_hub

lazy_import_rules:
  scope: "test files, fixtures, optional feature modules"

  must_lazy_import:
    - "All heavy_modules in test_*.py files"
    - "All heavy_modules in conftest.py fixtures"
    - "Optional dependencies in library code"

  allowed_module_level:
    - "pytest"
    - "typing"
    - "pathlib"
    - "dataclasses"
    - "Standard library lightweight modules"

  patterns:
    prohibited:
      location: "module level in test files"
      example: |
        import torch
        import numpy as np
        def test_foo(): ...

    required:
      location: "inside function/fixture body"
      example: |
        def test_foo():
            import torch
            import numpy as np
            ...

        @pytest.fixture
        def my_fixture():
            import torch
            return torch.randn(3, 224, 224)

test_file_rules:
  conftest_py:
    - "NO module-level torch/numpy imports"
    - "Import inside each fixture body"
    - "Use function-scoped fixtures when importing heavy modules"

  test_modules:
    - "Import heavy dependencies inside test functions"
    - "Use TYPE_CHECKING guards for type hints only"
    - "Group lazy imports at function start"

rationale_for_ai:
  objective: "Reduce pytest collection time"
  impact: "Heavy imports at module level execute during collection"
  target: "Collection time < execution overhead of lazy imports"

enforcement:
  detection: "grep for module-level imports in test files"
  future_hook: "pre-commit check for test file imports"
  command: |
    grep -rn "^import torch\|^import numpy\|^from torch\|^from numpy" tests/

# Tier 2 - Coding Standards: Pydantic Data Validation

pydantic_validation_rules:
  version: "pydantic v2"

  mandatory_locations:
    - "Dataset __getitem__ inputs/outputs"
    - "Transform pipeline input/output contracts"
    - "API request/response models"
    - "Configuration dataclasses"
    - "Cache serialization boundaries"

  prohibited_patterns:
    - "Passing raw dicts between pipeline stages"
    - "Unvalidated numpy array shapes"
    - "Tensor dtype assumptions without validation"
    - "Config values accessed without schema"

  required_features:
    config: "ConfigDict(arbitrary_types_allowed=True)"
    field_validation: "field_validator for shape/dtype checks"
    cross_field: "model_validator for cross-field validation"
    errors: "Descriptive ValidationError messages"

  patterns:
    prohibited:
      description: "Raw dict passing"
      example: |
        def process(data: dict) -> dict:
            return {"result": data["input"] * 2}

    required:
      description: "Pydantic model contracts"
      example: |
        class ProcessInput(BaseModel):
            model_config = ConfigDict(arbitrary_types_allowed=True)
            input: np.ndarray

            @field_validator("input")
            @classmethod
            def validate_shape(cls, v):
                if v.ndim != 2:
                    raise ValueError("Must be 2D")
                return v

        class ProcessOutput(BaseModel):
            result: np.ndarray

        def process(data: ProcessInput) -> ProcessOutput:
            return ProcessOutput(result=data.input * 2)

  reference_implementations:
    schemas: "ocr/datasets/schemas.py"
    contracts: "ocr/datasets/preprocessing/contracts.py"
    validation: "ocr/validation/models.py"
    transforms: "ocr/datasets/transforms.py"

# Tier 2 - Coding Standards: Error Handling

error_handling_rules:
  exception_hierarchy:
    base: "Use domain-specific exception classes"
    locations:
      - "ocr/exceptions.py for OCR-specific errors"
      - "Inherit from built-in exceptions (ValueError, TypeError)"

  required_patterns:
    - "Always include context in error messages"
    - "Use 'from exc' for exception chaining"
    - "Log before re-raising in boundaries"

  prohibited_patterns:
    - "Bare except clauses"
    - "Catching Exception without re-raise"
    - "Silent failures (empty except blocks)"
    - "String-only error messages without context"

  patterns:
    prohibited:
      example: |
        try:
            process(data)
        except:
            pass

    required:
      example: |
        try:
            process(data)
        except ValidationError as exc:
            logger.error("Processing failed for %s: %s", filename, exc)
            raise ProcessingError(f"Failed to process {filename}") from exc

  retry_logic:
    when: "Network calls, transient failures only"
    library: "tenacity"
    max_attempts: 3
    backoff: "exponential"

# Tier 2 - Coding Standards: Logging

logging_rules:
  levels:
    DEBUG: "Verbose diagnostic info (disabled in prod)"
    INFO: "Key operations, state changes"
    WARNING: "Recoverable issues, degraded performance"
    ERROR: "Failures requiring attention"
    CRITICAL: "System-wide failures"

  required_patterns:
    - "Use module-level logger: logger = logging.getLogger(__name__)"
    - "Use %-formatting for lazy evaluation: logger.info('x=%s', x)"
    - "Include relevant context (filename, batch_idx, etc.)"

  prohibited_patterns:
    - "print() for logging"
    - "f-strings in log calls (eager evaluation)"
    - "Logging secrets, passwords, API keys"
    - "Logging entire tensors or large arrays"

  patterns:
    prohibited:
      example: |
        print(f"Processing {filename}")
        logger.info(f"Result: {large_tensor}")

    required:
      example: |
        logger = logging.getLogger(__name__)
        logger.info("Processing file: %s", filename)
        logger.debug("Tensor shape: %s", tensor.shape)

# Tier 2 - Coding Standards: Type Annotations

type_annotation_rules:
  required_locations:
    - "All public function signatures"
    - "All class attributes"
    - "All module-level constants"

  prohibited_patterns:
    - "Using Any without justification"
    - "Missing return type annotations"
    - "Untyped **kwargs in public APIs"

  recommended_patterns:
    type_checking_guard:
      purpose: "Avoid import overhead for type hints"
      example: |
        from typing import TYPE_CHECKING
        if TYPE_CHECKING:
            import torch
            from ocr.models import DBHead

        def process(x: "torch.Tensor") -> "DBHead":
            import torch
            ...

    optional_types:
      pattern: "X | None instead of Optional[X]"
      example: "def process(data: np.ndarray | None = None)"

    collection_types:
      pattern: "list[X] instead of List[X]"
      example: "def process(items: list[str]) -> dict[str, int]"

  enforcement:
    tool: "mypy --strict"
    config: "pyproject.toml"
