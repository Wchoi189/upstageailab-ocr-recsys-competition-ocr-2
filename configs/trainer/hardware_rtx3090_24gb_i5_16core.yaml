# @package _global_

# Hardware-Optimized Configuration for RTX 3090 24GB + Intel i5 16-core
# Usage: python runners/train.py +trainer=hardware_rtx3090_24gb_i5_16core

defaults:
  - override /trainer: rtx3090_24gb
  - override /trainer: rtx3090_24gb

# Inline dataloader settings to avoid defaults conflict
data:
  dataloaders:
    train_dataloader:
      batch_size: ${batch_size}
      shuffle: true
      num_workers: 2
      pin_memory: true
      persistent_workers: true
      prefetch_factor: 3

    val_dataloader:
      batch_size: ${batch_size}
      shuffle: false
      num_workers: 2
      pin_memory: true
      persistent_workers: true
      prefetch_factor: 2

    test_dataloader:
      batch_size: ${batch_size}
      shuffle: false
      num_workers: 4
      pin_memory: true
      persistent_workers: true
      prefetch_factor: 2

    predict_dataloader:
      batch_size: ${batch_size}
      shuffle: false
      num_workers: 4
      pin_memory: true
      persistent_workers: true
      prefetch_factor: 2

# Recommended batch size for RTX 3090
batch_size: 8

datasets:
  train:
    config:
      preload_images: true
      load_maps: true
      cache_config:
        # With 24GB VRAM + 64GB System RAM, we can be aggressive with caching
        cache_transformed_tensors: false # Still unsafe for VRAM, but keep map/image caching
        cache_images: true
        cache_maps: true
        log_statistics_every_n: 100

  val_dataset:
    config:
      preload_images: true
      load_maps: true
