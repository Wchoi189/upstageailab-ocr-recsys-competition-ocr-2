# @package _global_

# Optimized Trainer Configuration for RTX 3060 12GB
# This configuration is optimized for RTX 3060 12GB GPU with Intel i5 16-core CPU
#
# Usage:
#   python runners/train.py trainer=rtx3060_12gb
#
# Hardware Specifications:
#   - GPU: RTX 3060 12GB VRAM
#   - CPU: Intel i5 16-core processor
#
# Performance Expectations:
#   - Baseline: ~250s/epoch
#   - Optimized: ~70-80s/epoch (3-3.5x speedup)
#   - Memory: 12GB VRAM with FP32 (full precision)
#
# Key Optimizations:
#   - FP32 full precision (no mixed precision due to compatibility)
#   - Gradient accumulation for effective larger batch sizes
#   - Optimized for single GPU training

trainer:
  max_steps: -1  # -1 means unlimited (controlled by max_epochs)
  max_epochs: 1
  num_sanity_val_steps: 1
  log_every_n_steps: 20
  val_check_interval: null
  check_val_every_n_epoch: 1
  deterministic: True

  # Gradient accumulation allows effective larger batch sizes
  # With batch_size=4 and accumulate_grad_batches=2, effective batch size = 8
  accumulate_grad_batches: 2

  # FP32 Full Precision Settings
  # Using FP32 (no mixed precision) for compatibility and stability
  # FP32 uses more memory but avoids mixed precision errors
  precision: "32-true"

  # Gradient clipping for training stability
  gradient_clip_val: 5.0
  gradient_clip_algorithm: "norm"

  # Performance Settings
  benchmark: false  # Enable cudnn.benchmark for speed

  # Device Configuration
  accelerator: gpu
  devices: 1
  strategy: auto  # auto, ddp, ddp_spawn, deepspeed

  # Dataloader Batch limits for debugging and performance testing (null = no limit)
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null

# IMPORTANT NOTES:
#
# 1. Batch Size Recommendations:
#    - Start with batch_size=4 for training (FP32 uses more memory)
#    - Can increase to 6 if memory allows
#    - Use accumulate_grad_batches=2 for effective batch size of 8-12
#
# 2. Memory Management:
#    - FP32 uses full precision, requiring more memory than FP16
#    - Monitor GPU memory usage with: nvidia-smi -l 1
#    - If OOM occurs, reduce batch_size or increase accumulate_grad_batches
#    - Consider disabling tensor caching to save memory
#
# 3. CPU Utilization:
#    - With 16 cores, use num_workers=12-14 for dataloaders
#    - Leave 2-4 cores for system processes
#
# 4. Performance Tuning:
#    - If training is slow, check GPU utilization (should be >90%)
#    - If GPU utilization is low, increase num_workers or prefetch_factor
#    - Consider enabling preload_images for faster data loading
#
# 5. Troubleshooting:
#    - If OOM errors occur, reduce batch_size or disable tensor caching
#    - Monitor for NaN/Inf values in early epochs
#    - FP32 is more stable than FP16 but uses more memory

