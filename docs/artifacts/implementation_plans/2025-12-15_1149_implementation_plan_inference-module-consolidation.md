---
ads_version: "1.0"
type: "implementation_plan"
category: "development"
status: "active"
version: "1.0"
tags: ['implementation', 'plan', 'development']
title: "Inference Module Consolidation - Implementation Plan"
date: "2025-12-15 11:49 (KST)"
branch: "refactor/inference-module-consolidation"
---

# Master Prompt

You are an autonomous AI agent, my Chief of Staff for implementing the **Inference Module Consolidation - Implementation Plan**. Your primary responsibility is to execute the "Living Implementation Blueprint" systematically, handle outcomes, and keep track of our progress. Do not ask for clarification on what to do next; your next task is always explicitly defined.

---

**Your Core Workflow is a Goal-Execute-Update Loop:**
1. **Goal:** A clear `ðŸŽ¯ Goal` will be provided for you to achieve.
2. **Execute:** You will start working on the task defined in the `NEXT TASK`
3. **Handle Outcome & Update:** Based on the success or failure of the command, you will follow the specified contingency plan. Your response must be in two parts:
   * **Part 1: Execution Report:** Provide a concise summary of the results and analysis of the outcome (e.g., "All tests passed" or "Test X failed due to an IndexError...").
   * **Part 2: Blueprint Update Confirmation:** Confirm that the living blueprint has been updated with the new progress status and next task. The updated blueprint is available in the workspace file.

---

# Living Implementation Blueprint: Inference Module Consolidation - Implementation Plan

## Progress Tracker
- **STATUS:** Complete - Engine Refactoring Phase (Phase 3.2)
- **CURRENT STEP:** Phase 4 - Documentation Updates
- **LAST COMPLETED TASK:** Phase 3.2 - Migrate InferenceEngine Methods (Commit: bd258a4)
- **NEXT TASK:** Phase 3.3 - Update Configuration Loading (Optional)

---

## ðŸŽ¯ **Scope and Context**

**Problem Statement:** The `ocr/inference/engine.py` module has grown to 899 lines (755 lines in `InferenceEngine` class alone), with a 363-line `_predict_impl` method handling 10+ responsibilities. This creates maintenance burden and prevents efficient AI code analysis.

**Solution Strategy:** Modular refactoring with clean migrations (no backward compatibility). Consolidate duplicate image processing logic, eliminate over-engineered exception clauses, and create focused, testable components.

**Key Constraints:**
- No backward compatibility required
- Clean migration preferred over deprecation paths
- Consolidate duplicate perspective correction implementations
- Remove over-engineered exception handling

### Implementation Outline (Checklist)

#### **Phase 1: Extraction Foundation (Low Risk, 1-2 days)**
Parallel extraction of utilities to reduce duplication and establish reusable components.

1. [x] **Task 1.0: Repository and Branch Setup**
   - [x] Checkout to new branch: `refactor/inference-module-consolidation`
   - [x] Create AgentQMS implementation plan artifact

2. [x] **Task 1.1: Extract CoordinateTransformationManager** (Priority 1) âœ… COMPLETED
   - [x] Create new file: `ocr/inference/coordinate_manager.py`
   - [x] Extract `compute_inverse_matrix()` logic from postprocess.py
   - [x] Extract polygon transformation from `_map_polygons_to_preview_space()` (engine.py:642-729)
   - [x] Create unified transformation API with tests
   - [x] Update engine.py and postprocess.py to use new manager
   - [x] Verify no regression in polygon coordinate handling
   - **Results:** 45/45 unit tests passed, 6/6 integration tests passed, ~88 lines removed from engine.py
   - **Commit:** 93ac029

3. [x] **Task 1.2: Extract Metadata Calculation Utilities** (Priority 2) âœ… COMPLETED
   - [x] Create new file: `ocr/inference/preprocessing_metadata.py`
   - [x] Consolidate duplicate metadata calculations from engine.py (lines 502-542, 612-631)
   - [x] Create pure functions: `create_preprocessing_metadata()`, `calculate_resize_dimensions()`, `calculate_padding()`, `get_content_area()`
   - [x] Add comprehensive tests for edge cases
   - [x] Update engine.py to use utilities (3 duplicate blocks replaced)
   - [x] Verify metadata consistency across code paths
   - **Results:** 30/30 unit tests passed, 6/6 integration tests passed, ~45 lines removed from engine.py
   - **Commit:** d2bdea4

4. [x] **Task 1.3: Extract Preview Generation** (Priority 3) âœ… COMPLETED
   - [x] Create new file: `ocr/inference/preview_generator.py`
   - [x] Extract preview image generation and encoding logic
   - [x] Create `PreviewGenerator` class with configurable JPEG quality
   - [x] Extract polygon transformation to preview space (now in PreviewGenerator)
   - [x] Update engine.py to use PreviewGenerator (removed nested functions)
   - [x] Add comprehensive tests for encoding, attachment, transformations
   - **Results:** 31/31 unit tests passed, ~40 lines removed from engine.py
   - **Commit:** 46563fa

5. [x] **Task 1.4: Extract Image I/O Handler** (Priority 4) âœ… COMPLETED
   - [x] Create new file: `ocr/inference/image_loader.py`
   - [x] Extract `_load_image()` method logic (engine.py:329-406)
   - [x] Extract EXIF handling and orientation normalization
   - [x] Consolidate with `ocr/utils/orientation.py` for redundancy check
   - [x] Create `ImageLoader` class with format conversion
   - [x] Add comprehensive tests for various image formats
   - [x] Verify compatibility with PIL, OpenCV, numpy arrays
   - **Results:** 26/26 unit tests passed, 132/132 Phase 1 tests passed, ~20 lines removed from engine.py
   - **Commit:** 6558f11

6. [x] **Task 1.5: Audit and Consolidate Duplicate Perspective Correction** (Priority 5) âœ… AUDIT COMPLETE (Consolidation Deferred)
   - [x] Review perspective correction implementations:
     - `ocr/utils/perspective_correction.py` (460 lines)
     - `ocr/datasets/preprocessing/perspective.py` (108 lines)
     - Inference module usage (via preprocess.py)
     - Datasets module usage (via preprocessing pipeline)
   - [x] Identify over-engineered exception clauses and failed fix attempts
   - [x] Create audit document of differences
   - [ ] **DEFERRED:** Consolidate to single implementation in `ocr/preprocessing/perspective_correction.py` (new)
   - [ ] **DEFERRED:** Create migration path for datasets and inference modules
   - **Results:** Audit complete, 86 files affected, consolidation scope too large for Phase 1
   - **Recommendation:** Defer consolidation to dedicated phase (Phase 2+)
   - **Artifact:** docs/artifacts/audits/2025-12-15_perspective_correction_consolidation_audit.md

#### **Phase 2: Pipeline Classes (Medium Risk, 2-3 days)**
Creation of focused pipeline classes for clear separation of concerns.

7. [x] **Task 2.1: Create PreprocessingPipeline** (Priority 1) âœ… COMPLETED
   - [x] Create new file: `ocr/inference/preprocessing_pipeline.py`
   - [x] Integrate extracted utilities (coordinate_manager, preprocessing_metadata)
   - [x] Create stages: perspective correction â†’ resize/pad â†’ normalize â†’ metadata
   - [x] Consolidate logic from `_predict_impl()` (engine.py:418-509)
   - [x] Add comprehensive unit tests for each stage
   - [x] Create PreprocessingResult dataclass for structured output
   - [x] Add from_settings() factory method for easy initialization
   - **Results:** 12 unit tests written (dependencies required for execution), ~260 lines pipeline module
   - **Commit:** TBD

8. [x] **Task 2.2: Create PostprocessingPipeline** (Priority 2) âœ… COMPLETED
   - [x] Create new file: `ocr/inference/postprocessing_pipeline.py`
   - [x] Verify postprocess.py already uses coordinate_manager (completed in Phase 1.1)
   - [x] Create stages: decode (head/fallback) â†’ transform coordinates â†’ format output
   - [x] Add comprehensive unit tests
   - [x] Create PostprocessingResult dataclass for structured output
   - **Results:** 9/9 unit tests passed, ~140 lines pipeline module
   - **Commit:** TBD

9. [x] **Task 2.3: Create ModelManager** (Priority 3) âœ… COMPLETED
   - [x] Create new file: `ocr/inference/model_manager.py`
   - [x] Extract model lifecycle from engine.py: load, cleanup, state management
   - [x] Extract model loading logic (engine.py:151-225)
   - [x] Integrate checkpoint loading from model_loader.py
   - [x] Create unified model management API with caching
   - [x] Add state management tests and context manager support
   - **Results:** 13/13 unit tests passed, ~230 lines manager module
   - **Commit:** TBD

#### **Phase 3: Engine Refactoring (High Risk, 3-5 days)**
Split `InferenceEngine` into focused classes following orchestrator pattern.

10. [x] **Task 3.1: Create InferenceOrchestrator Base** (Priority 1) âœ… COMPLETED
    - [x] Create new file: `ocr/inference/orchestrator.py`
    - [x] Define orchestrator interface with clear responsibilities
    - [x] Integrate: ModelManager + PreprocessingPipeline + PostprocessingPipeline + PreviewGenerator
    - [x] Create coordination layer for `predict()` method (5-stage pipeline)
    - [x] Add configuration management and parameter updates
    - [x] Add unit tests (integration tests deferred)
    - **Results:** 10/10 unit tests passed, ~260 lines orchestrator module
    - **Commit:** TBD

11. [x] **Task 3.2: Migrate InferenceEngine Methods** (Priority 2) âœ… COMPLETED
    - [x] Migrate `predict_array()` to orchestrator (simplify to dispatcher)
    - [x] Migrate `predict_image()` to orchestrator
    - [x] Update internal state management
    - [x] Add migration tests
    - **Results:** engine.py reduced from 899 to 298 lines (-67%), 164/176 tests passing
    - **Commit:** bd258a4

12. [ ] **Task 3.3: Update Configuration Loading** (Priority 3)
    - [ ] Review and simplify `config_loader.py`
    - [ ] Apply strategy pattern for JSON vs Hydra configs
    - [ ] Reduce fallback chain from 10+ to 3-4 locations
    - [ ] Add configuration validation
    - [ ] Add comprehensive tests for edge cases

13. [ ] **Task 3.4: Update All Call Sites** (Priority 4)
    - [ ] Identify all imports of InferenceEngine
    - [ ] Update imports to use new classes
    - [ ] Update instantiation code
    - [ ] Update method calls
    - [ ] Run integration tests after each update

#### **Phase 4: Consolidation & Cleanup (Medium Risk, 2-3 days)**
Unify duplicate implementations and remove over-engineering.

14. [ ] **Task 4.1: Consolidate Perspective Correction** (Priority 1)
    - [ ] Complete migration from old implementations
    - [ ] Remove duplicate code in datasets module
    - [ ] Update imports and call sites
    - [ ] Run comprehensive tests across modules
    - [ ] Document consolidation in CHANGELOG

15. [ ] **Task 4.2: Audit and Fix Over-Engineering** (Priority 2)
    - [ ] Review exception handling patterns across module
    - [ ] Identify over-engineered exception clauses (from audit in 1.5)
    - [ ] Simplify exception handling to be maintainable
    - [ ] Document failed fix attempts and decisions
    - [ ] Add tests for error cases

16. [ ] **Task 4.3: Remove Dead Code** (Priority 3)
    - [ ] Identify unused methods in original InferenceEngine
    - [ ] Remove or consolidate with new implementations
    - [ ] Clean up any temporary adapter code
    - [ ] Update imports accordingly

#### **Phase 5: Testing & Validation (1-2 days)**
Comprehensive testing to ensure correctness and performance.

17. [ ] **Task 5.1: Unit Test Suite** (Priority 1)
    - [ ] Add unit tests for CoordinateTransformationManager
    - [ ] Add unit tests for PreprocessingPipeline
    - [ ] Add unit tests for PostprocessingPipeline
    - [ ] Add unit tests for ImageLoader
    - [ ] Add unit tests for PreviewGenerator
    - [ ] Target: >90% code coverage for new modules

18. [ ] **Task 5.2: Integration Test Suite** (Priority 2)
    - [ ] Update existing integration tests in `tests/integration/`
    - [ ] Add tests for full inference pipeline (preprocessing â†’ model â†’ postprocessing)
    - [ ] Add tests for coordinate transformation consistency
    - [ ] Add tests for image format handling
    - [ ] Verify tests still pass with refactored code

19. [ ] **Task 5.3: Regression Testing** (Priority 3)
    - [ ] Run full test suite: `pytest tests/`
    - [ ] Verify output consistency with original implementation
    - [ ] Check for any performance regressions
    - [ ] Validate with real inference data if available

20. [ ] **Task 5.4: Documentation and Handover** (Priority 4)
    - [ ] Update docstrings for all new modules
    - [ ] Create architecture diagram
    - [ ] Document module responsibilities and dependencies
    - [ ] Update README with new structure
    - [ ] Prepare session handover document

---

## ðŸ“‹ **Technical Requirements Checklist**

### **Architecture & Design**
- [ ] **Modular Design:** Split InferenceEngine into focused, single-responsibility classes
- [ ] **Dependency Injection:** All components receive dependencies explicitly (not globals)
- [ ] **Pure Functions:** Coordinate transformations and metadata calculations as pure functions
- [ ] **Type Hints:** Full type annotation on all public APIs (Python 3.9+)
- [ ] **Clean Code:** Max method length: 50 lines; max class: 300 lines
- [ ] **Configuration:** Unified config loading with strategy pattern

### **Integration Points**
- [ ] **Backward Compatibility:** NOT required (clean migration preferred)
- [ ] **Existing Utilities:** Consolidate with `ocr/utils/orientation.py`, `ocr/utils/perspective_correction.py`
- [ ] **Model Integration:** ModelManager integrates with existing checkpoint system
- [ ] **Preprocessing Integration:** PreprocessingPipeline integrates with `ocr/inference/preprocess.py`
- [ ] **Postprocessing Integration:** PostprocessingPipeline integrates with `ocr/inference/postprocess.py`
- [ ] **Configuration Loading:** Update config_loader.py to remove complex fallback chains

### **Quality Assurance**
- [ ] **Unit Test Coverage:** >90% for new modules
- [ ] **Integration Tests:** Full pipeline tests with real images
- [ ] **Regression Tests:** Output consistency validation against baseline
- [ ] **Performance Tests:** No latency increase from refactoring
- [ ] **Code Review:** Technical review for architecture decisions
- [ ] **Documentation:** Complete API documentation for new classes

---

## ðŸŽ¯ **Success Criteria Validation**

### **Functional Requirements**
- [ ] All inference operations work identically to original implementation
- [ ] Polygon coordinates map correctly between original and processed spaces
- [ ] Image preprocessing produces identical output
- [ ] Model predictions remain consistent
- [ ] Postprocessing produces identical results

### **Technical Requirements**
- [ ] engine.py reduced from 899 to <400 lines
- [ ] InferenceEngine class reduced from 755 to <200 lines (orchestrator wrapper)
- [ ] _predict_impl method eliminated (logic distributed to pipelines)
- [ ] Zero code duplication in coordinate transformation logic
- [ ] All modules <300 lines with single clear responsibility
- [ ] 100% type hint coverage on public APIs
- [ ] Test suite passes with >90% coverage
- [ ] No performance regression in inference latency

### **Code Quality Metrics**
- [ ] Cyclomatic complexity <15 for all methods
- [ ] Average method length <30 lines
- [ ] Average class size <250 lines
- [ ] Clear separation of concerns: preprocessing, inference, postprocessing
- [ ] Dependency graph is acyclic (no circular imports)

---

## ðŸ”§ **Detailed Task Breakdown and Execution Guide**

### **Phase 1.1: Extract CoordinateTransformationManager**

**Rationale:** Coordinates are transformed multiple times in different parts of the system. Currently:
- `compute_inverse_matrix()` in postprocess.py handles transformation
- `_map_polygons_to_preview_space()` in engine.py (88 lines) duplicates logic
- Nested function `_coordinate_transform_to_preview_space()` in engine.py (59 lines) repeats transformation

**Current Code Locations:**
- Transformation logic: [postprocess.py#compute_inverse_matrix](ocr/inference/postprocess.py)
- Duplication: [engine.py#L642-L729](ocr/inference/engine.py#L642-L729)
- Nested functions: [engine.py#L679-L746](ocr/inference/engine.py#L679-L746)

**Execution Steps:**
1. Create `ocr/inference/coordinate_manager.py`
2. Extract transformation matrix calculation to pure function
3. Extract polygon mapping logic
4. Create CoordinateTransformationManager class to manage state and transformations
5. Add comprehensive tests for various scenarios (rotated images, scaled images, etc.)
6. Replace all duplicates with manager calls
7. Verify all existing tests still pass

**Expected Changes:**
- New file: `ocr/inference/coordinate_manager.py` (~150 lines)
- Refactored: `ocr/inference/engine.py` (remove ~88 lines from nested function)
- Refactored: `ocr/inference/postprocess.py` (simplify to use manager)
- New tests: `tests/unit/test_coordinate_manager.py` (~200 lines)

---

### **Phase 1.2: Extract Metadata Calculation Utilities**

**Rationale:** Metadata calculation is duplicated:
- Initial calculation in `_predict_impl()` at lines 556-570
- Recalculation for original image at lines 602-610
- Similar calculations in postprocess.py

**Current Duplication Pattern:**
```python
# Location 1: lines 556-570
max_side = max(original_h, original_w)
scale = target_size / max_side if max_side > 0 else 1.0
resized_h = int(round(original_h * scale))
resized_w = int(round(original_w * scale))
pad_h = target_size - resized_h
pad_w = target_size - resized_w

# Location 2: lines 602-610 (same logic)
max_side = max(original_h, original_w)
scale = target_size / max_side if max_side > 0 else 1.0
# ... repeated code
```

**Execution Steps:**
1. Create `ocr/inference/preprocessing_metadata.py`
2. Extract pure function: `calculate_resize_metadata(h, w, target_size) â†’ dict`
3. Extract pure function: `calculate_transformation_metadata(original, processed, transform_matrix) â†’ dict`
4. Add validation and edge case handling
5. Write tests for edge cases (zero size, very small/large images)
6. Replace duplicates with function calls
7. Verify metadata consistency

**Expected Changes:**
- New file: `ocr/inference/preprocessing_metadata.py` (~80 lines)
- Refactored: `ocr/inference/engine.py` (remove ~60 lines of duplicated logic)
- New tests: `tests/unit/test_preprocessing_metadata.py` (~150 lines)

---

### **Phase 1.3-1.5: Other Foundation Extractions**

Similar extraction process for:
- **Preview Generation** (engine.py:733-769, 59 lines nested function)
- **Image Loading** (engine.py:329-406, 76 lines with EXIF handling)
- **Perspective Correction Consolidation** (audit duplicate implementations)

Each follows same pattern:
1. Identify duplication/complexity
2. Extract to new focused class/module
3. Write comprehensive tests
4. Update call sites
5. Verify no regressions

---

## ðŸ“Œ **Critical Dependencies and Milestones**

### **Dependency Chain:**
```
Phase 1 (Extractions)
  â†“
  â””â”€â†’ CoordinateTransformationManager (needed by Phase 2)
  â””â”€â†’ PreprocessingMetadata (needed by Phase 2)
  â””â”€â†’ ImageLoader (independent)
  â””â”€â†’ PreviewGenerator (independent)
  â†“
Phase 2 (Pipelines - can start once Phase 1.1 & 1.2 complete)
  â†“
  â””â”€â†’ PreprocessingPipeline (uses CoordinateTransformationManager)
  â””â”€â†’ PostprocessingPipeline (uses CoordinateTransformationManager)
  â””â”€â†’ ModelManager (independent)
  â†“
Phase 3 (Engine Refactoring - requires Phase 2 complete)
  â†“
Phase 4 (Consolidation - parallel with remaining phases)
```

### **Risk Milestones:**
- **Low Risk:** Completing Phase 1 (extractions are isolated, can be reverted)
- **Medium Risk:** Phase 2 (pipelines introduce new abstraction layer)
- **High Risk:** Phase 3 (changes core orchestration logic)
- **Validation:** Phase 5 (critical to run full test suite)

---

## âš ï¸ **Contingency Plans**

### **If Phase 1 extraction breaks existing tests:**
1. Check if extracted function signature matches original expectations
2. Add adapter/compatibility layer if needed
3. Review test for hidden dependencies
4. Roll back extraction and re-evaluate design

### **If Phase 2 pipeline creates performance regression:**
1. Profile with real inference data
2. Check for unnecessary object creation
3. Validate that numpy operations are efficient
4. Consider caching transformation matrices

### **If Phase 3 refactoring causes inference failures:**
1. Enable verbose logging to trace execution
2. Compare pipeline outputs at each stage
3. Isolate failure to specific pipeline component
4. Roll back last pipeline change and debug

### **Recovery Strategy:**
All changes are committed to git with clear messages. If major issues arise:
1. Identify problematic commit with git bisect
2. Create hotfix branch from working commit
3. Fix issue and merge back to main refactor branch
4. Update progress tracker and continue

---

## ðŸ“ **Session Handover Protocol**

**When to trigger new session:**
- After completing 2 major phases (~2-3 days of work)
- When context token usage exceeds 120k tokens
- When async compilation/testing takes >5 minutes
- When new architectural questions emerge

**Handover includes:**
1. **Current Progress:** Updated checklist in this document
2. **Next Task:** Explicitly defined with code references
3. **Key Context:**
   - Modified files list
   - New files created
   - Test status summary
4. **Relevant Code Snippets:** Key changes that affect next phase
5. **Known Issues:** Any open problems or incomplete work
6. **Documentation References:**
   - Original analysis: Comprehensive refactoring analysis report
   - Architecture diagram (if created): TBD
   - Test results summary

**Handover Trigger Statement:**
```
CONTEXT SATURATED - Ready for Session Handover
Session Token Usage: [X]/200k
Completed Phases: [X of 5]
Next Tasks: [Explicitly listed]
Branch: refactor/inference-module-consolidation
Artifact: This document (continuously updated)
```

---

## ðŸŽ¬ **Getting Started**

### **Before first execution:**
1. âœ… Branch created: `refactor/inference-module-consolidation`
2. âœ… AgentQMS artifact created with detailed breakdown
3. Run preliminary analysis: `cd AgentQMS/interface && make validate`

### **Execute Phase 1 by starting with:**
```bash
cd /workspaces/upstageailab-ocr-recsys-competition-ocr-2

# Checkpoint baseline
git status
pytest tests/integration/test_inference.py -v  # Verify baseline

# Begin Phase 1.1: Extract CoordinateTransformationManager
# [Execute tasks as defined above]
```

### **After each task completion:**
1. Update checklist in this document
2. Commit changes with clear message: `git commit -m "Phase X.Y: Extract [component]"`
3. Run affected tests: `pytest tests/unit/test_[component].py -v`
4. Update "NEXT TASK" in Progress Tracker

---

## ðŸ“š **Reference Documentation**

**Original Analysis Document:** Generated comprehensive refactoring analysis identifying:
- God Object anti-pattern in InferenceEngine (10+ responsibilities)
- Monster method `_predict_impl()` (363 lines, cyclomatic complexity 20+)
- Duplicate metadata calculations (2+ locations)
- Duplicate perspective correction implementations (3+ locations)

**Key Code Locations:**
- Main class: [ocr/inference/engine.py#InferenceEngine](ocr/inference/engine.py)
- Monster method: [ocr/inference/engine.py#L406-L769](ocr/inference/engine.py#L406-L769)
- Coordinate duplication: [ocr/inference/engine.py#L642-L729](ocr/inference/engine.py#L642-L729)
- Metadata duplication: [ocr/inference/engine.py#L556-L610](ocr/inference/engine.py#L556-L610)
- PostProcessing: [ocr/inference/postprocess.py](ocr/inference/postprocess.py)
- Config Loading: [ocr/inference/config_loader.py](ocr/inference/config_loader.py)
- Perspective Correction: [ocr/utils/perspective_correction.py](ocr/utils/perspective_correction.py)

**Related Code:**
- Orientation handling: [ocr/utils/orientation.py](ocr/utils/orientation.py)
- Dataset preprocessing: [ocr/datasets/](ocr/datasets/) (shared perspective correction)
- Preprocessing utilities: [ocr/inference/preprocess.py](ocr/inference/preprocess.py)

---

**Document Status:** Living document - continuously updated as work progresses
**Last Updated:** 2025-12-15 11:49 KST
**Branch:** `refactor/inference-module-consolidation`
**Next Review:** After Phase 1 completion

---

## ðŸ“Š **Risk Mitigation & Fallbacks**

### **Current Risk Level**: LOW / MEDIUM / HIGH
### **Active Mitigation Strategies**:
1. [Mitigation Strategy 1 (e.g., Incremental Development)]
2. [Mitigation Strategy 2 (e.g., Comprehensive Testing)]
3. [Mitigation Strategy 3 (e.g., Regular Code Quality Checks)]

### **Fallback Options**:
1. [Fallback Option 1 if Risk A occurs (e.g., Simplified version of a feature)]
2. [Fallback Option 2 if Risk B occurs (e.g., CPU-only mode)]
3. [Fallback Option 3 if Risk C occurs (e.g., Phased Rollout)]

---

## ðŸ”„ **Blueprint Update Protocol**

**Update Triggers:**
- Task completion (move to next task)
- Blocker encountered (document and propose solution)
- Technical discovery (update approach if needed)
- Quality gate failure (address issues before proceeding)

**Update Format:**
1. Update Progress Tracker (STATUS, CURRENT STEP, LAST COMPLETED TASK, NEXT TASK)
2. Mark completed items with [x]
3. Add any new discoveries or changes to approach
4. Update risk assessment if needed

---

## ðŸš€ **Immediate Next Action**

**TASK:** [Description of the immediate next task]

**OBJECTIVE:** [Clear, concise goal of the task]

**APPROACH:**
1. [Step 1 to execute the task]
2. [Step 2 to execute the task]
3. [Step 3 to execute the task]

**SUCCESS CRITERIA:**
- [Measurable outcome 1 that defines task completion]
- [Measurable outcome 2 that defines task completion]

---

*This implementation plan follows the Blueprint Protocol Template (PROTO-GOV-003) for systematic, autonomous execution with clear progress tracking.*
