# ==============================================================================
# RESOLVED CONFIGURATION: recognition
# ==============================================================================
# This file shows the FINAL resolved values after all interpolations.
# Use this for AI agent context to eliminate ${variable} guessing.
# ==============================================================================

mode: train
global:
  paths:
    root_dir: .
    data_dir: ./data
    output_dir: ./outputs
    datasets_root: ./data/datasets
    artifacts:
      detection: ./outputs/detection
      recognition: ./outputs/recognition
      kie: ./outputs/kie
    checkpoints: ./outputs/checkpoints
    logs: ./outputs/logs
    wandb: ./outputs/wandb
    datasets:
      train: ./data/train
      val: ./data/val
      test: ./data/test
    cache: ./outputs/cache
    temp: ./outputs/temp
  default_interpolation: 1
trainer:
  max_steps: -1
  max_epochs: 200
  num_sanity_val_steps: 1
  log_every_n_steps: 20
  val_check_interval: null
  check_val_every_n_epoch: 1
  deterministic: false
  accumulate_grad_batches: 2
  precision: 32-true
  benchmark: true
  gradient_clip_val: 5.0
  accelerator: gpu
  devices: 1
  strategy: auto
  enable_checkpointing: false
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
dataloaders:
  train_dataloader:
    num_workers: 12
    pin_memory: true
    persistent_workers: true
    prefetch_factor: 3
  val_dataloader:
    num_workers: 8
    pin_memory: true
    persistent_workers: true
    prefetch_factor: 2
  test_dataloader:
    num_workers: 4
    pin_memory: true
    persistent_workers: true
    prefetch_factor: 1
  predict_dataloader:
    num_workers: 4
    pin_memory: true
    persistent_workers: true
    prefetch_factor: 1
hardware:
  gpu: rtx3060
  vram: 12gb
  cpu: i5_16core
data:
  batch_size: 128
  transforms:
    _target_: torchvision.transforms.Compose
    transforms:
    - _target_: torchvision.transforms.Resize
      size:
      - 224
      - 224
    - _target_: torchvision.transforms.ToTensor
    - _target_: torchvision.transforms.Normalize
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
  num_workers: 4
  tokenizer:
    _target_: ocr.features.recognition.data.tokenizer.KoreanOCRTokenizer
    charset_path: ./ocr/data/charset.json
    max_len: 25
  _dataset_template:
    _target_: ocr.features.recognition.data.lmdb_dataset.LMDBRecognitionDataset
    lmdb_path: ./data/train/recognition/aihub_lmdb_validation
    tokenizer:
      _target_: ocr.features.recognition.data.tokenizer.KoreanOCRTokenizer
      charset_path: ./ocr/data/charset.json
      max_len: 25
    max_len: 25
    transform:
      _target_: torchvision.transforms.Compose
      transforms:
      - _target_: torchvision.transforms.Resize
        size:
        - 224
        - 224
      - _target_: torchvision.transforms.ToTensor
      - _target_: torchvision.transforms.Normalize
        mean:
        - 0.485
        - 0.456
        - 0.406
        std:
        - 0.229
        - 0.224
        - 0.225
    config: null
  datasets:
    train_dataset:
      _target_: ocr.features.recognition.data.lmdb_dataset.LMDBRecognitionDataset
      lmdb_path: ./data/train/recognition/aihub_lmdb_validation
      tokenizer:
        _target_: ocr.features.recognition.data.tokenizer.KoreanOCRTokenizer
        charset_path: ./ocr/data/charset.json
        max_len: 25
      max_len: 25
      transform:
        _target_: torchvision.transforms.Compose
        transforms:
        - _target_: torchvision.transforms.Resize
          size:
          - 224
          - 224
        - _target_: torchvision.transforms.ToTensor
        - _target_: torchvision.transforms.Normalize
          mean:
          - 0.485
          - 0.456
          - 0.406
          std:
          - 0.229
          - 0.224
          - 0.225
      config: null
    val_dataset:
      _target_: ocr.features.recognition.data.lmdb_dataset.LMDBRecognitionDataset
      lmdb_path: ./data/train/recognition/aihub_lmdb_validation
      tokenizer:
        _target_: ocr.features.recognition.data.tokenizer.KoreanOCRTokenizer
        charset_path: ./ocr/data/charset.json
        max_len: 25
      max_len: 25
      transform:
        _target_: torchvision.transforms.Compose
        transforms:
        - _target_: torchvision.transforms.Resize
          size:
          - 224
          - 224
        - _target_: torchvision.transforms.ToTensor
        - _target_: torchvision.transforms.Normalize
          mean:
          - 0.485
          - 0.456
          - 0.406
          std:
          - 0.229
          - 0.224
          - 0.225
      config: null
    test_dataset:
      _target_: ocr.features.recognition.data.lmdb_dataset.LMDBRecognitionDataset
      lmdb_path: ./data/train/recognition/aihub_lmdb_validation
      tokenizer:
        _target_: ocr.features.recognition.data.tokenizer.KoreanOCRTokenizer
        charset_path: ./ocr/data/charset.json
        max_len: 25
      max_len: 25
      transform:
        _target_: torchvision.transforms.Compose
        transforms:
        - _target_: torchvision.transforms.Resize
          size:
          - 224
          - 224
        - _target_: torchvision.transforms.ToTensor
        - _target_: torchvision.transforms.Normalize
          mean:
          - 0.485
          - 0.456
          - 0.406
          std:
          - 0.229
          - 0.224
          - 0.225
      config: null
    predict_dataset:
      _target_: ocr.features.recognition.data.lmdb_dataset.LMDBRecognitionDataset
      lmdb_path: ./data/train/recognition/aihub_lmdb_validation
      tokenizer:
        _target_: ocr.features.recognition.data.tokenizer.KoreanOCRTokenizer
        charset_path: ./ocr/data/charset.json
        max_len: 25
      max_len: 25
      transform:
        _target_: torchvision.transforms.Compose
        transforms:
        - _target_: torchvision.transforms.Resize
          size:
          - 224
          - 224
        - _target_: torchvision.transforms.ToTensor
        - _target_: torchvision.transforms.Normalize
          mean:
          - 0.485
          - 0.456
          - 0.406
          std:
          - 0.229
          - 0.224
          - 0.225
      config: null
  collate_fn:
    _target_: ocr.data.datasets.recognition_collate_fn.recognition_collate_fn
    _partial_: true
model:
  architectures:
    _target_: ocr.domains.recognition.models.PARSeq
    name: parseq
    encoder:
      _target_: ocr.core.models.encoder.TimmBackbone
      model_name: resnet18
      pretrained: true
    decoder:
      _target_: ocr.domains.recognition.models.decoder.PARSeqDecoder
      d_model: 512
      nhead: 8
      num_layers: 12
    max_len: 25
_group_:
  model:
    vocab_size: 1000
    max_len: 25
  detection: null
  max_polygons: null
  shrink_ratio: null
  thresh_min: null
  thresh_max: null
  box_thresh: null
  unclip_ratio: null
  kie: null
  max_entities: null
  relation_types: null
  entity_categories: null
  linking_mode: null
  recognition:
    max_label_length: 25
    charset: korean
    case_sensitive: false
    decode_mode: greedy
    beam_width: 5
    use_eos_token: true
    use_pad_token: true
  domain_info:
    name: recognition
    description: Korean text recognition using PARSeq architecture
    version: '2.0'
    required_keys:
    - recognition.max_label_length
    - recognition.charset
    forbidden_keys:
    - max_polygons
    - max_entities
runtime:
  caching: false
  prefetch: false
