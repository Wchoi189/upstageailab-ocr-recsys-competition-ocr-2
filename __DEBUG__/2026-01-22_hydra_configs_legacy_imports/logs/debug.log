DEBUG: OCRModel loaded from: /workspaces/upstageailab-ocr-recsys-competition-ocr-2/ocr/core/models/architecture.py
DEBUG: PARSeq MRO: [<class 'ocr.domains.recognition.models.architecture.PARSeq'>, <class 'ocr.core.models.architecture.OCRModel'>, <class 'torch.nn.modules.module.Module'>, <class 'object'>]
DEBUG: PARSeq.get_optimizers:     def get_optimizers(self):
        raise RuntimeError("I AM THE MODIFIED CODE")
        if "optimizer" in self.cfg:
            optimizer_config = self.cfg.optimizer
        elif "model" in self.cfg and "optimizer" in self.cfg.model:
            optimizer_config = self.cfg.model.optimizer
        elif "train" in self.cfg and "optimizer" in self.cfg.train:
             optimizer_config = self.cfg.train.optimizer
        else:
            # Fallback or error prompt
            try:
                optimizer_config = self.cfg.optimizer
            except Exception:
                 import omegaconf
                 # Debug: print available keys to find where it is
                 logger.error(f"DEBUG: Available keys in cfg: {list(self.cfg.keys())}")
                 if "model" in self.cfg:
                      logger.error(f"DEBUG: Available keys in cfg.model: {list(self.cfg.model.keys())}")
                 raise omegaconf.errors.ConfigAttributeError("Missing key optimizer (checked root, model.optimizer, train.optimizer)")

        # Manual instantiation fallback for Adam/AdamW to workaround instantiate() issues
        target = optimizer_config.get("_target_", "")
        print(f"DEBUG: TARGET IS '{target}' TYPE: {type(target)}")
        print(f"DEBUG: CONFIG IS {optimizer_config}")

        if "AdamW" in str(target):
             optimizer = torch.optim.AdamW(
                self.parameters(),
                lr=optimizer_config.get("lr", 0.001),
                betas=optimizer_config.get("betas", (0.9, 0.999)),
                eps=optimizer_config.get("eps", 1e-8),
                weight_decay=optimizer_config.get("weight_decay", 0.01)
            )
        elif "Adam" in str(target):
            optimizer = torch.optim.Adam(
                self.parameters(),
                lr=optimizer_config.get("lr", 0.001),
                betas=optimizer_config.get("betas", (0.9, 0.999)),
                eps=optimizer_config.get("eps", 1e-8),
                weight_decay=optimizer_config.get("weight_decay", 0)
            )
        else:
            print(f"DEBUG: Falling back to instantiate for target: '{target}'")
            try:
                optimizer = instantiate(optimizer_config, params=self.parameters())
            except TypeError as e:
                print(f"DEBUG: Failed to instantiate: {e}")
                raise e

        scheduler = None
        if "scheduler" in self.cfg:
            scheduler_config = self.cfg.scheduler
            scheduler = instantiate(scheduler_config, optimizer=optimizer)

        return [optimizer], [scheduler] if scheduler else []

[2026-01-22 08:39:00,068][ocr.pipelines.orchestrator][INFO] - üéØ OCRProjectOrchestrator initialized
[2026-01-22 08:39:00,069][ocr.pipelines.orchestrator][INFO] -    Domain: recognition
[2026-01-22 08:39:00,070][ocr.pipelines.orchestrator][INFO] -    Mode: train
[2026-01-22 08:39:00,070][ocr.pipelines.orchestrator][INFO] - 
============================================================
[2026-01-22 08:39:00,071][ocr.pipelines.orchestrator][INFO] - üöÄ Starting TRAIN for recognition domain
[2026-01-22 08:39:00,072][ocr.pipelines.orchestrator][INFO] - ============================================================

[2026-01-22 08:39:00,073][ocr.pipelines.orchestrator][INFO] - üèóÔ∏è Building model and datasets...
[2026-01-22 08:39:00,073][ocr.pipelines.orchestrator][INFO] - üíâ Injecting vocab_size for recognition model...
[2026-01-22 08:39:00,118][ocr.domains.recognition.data.tokenizer][INFO] - Loaded tokenizer: 1023 chars, vocab_size=1027, max_len=25
[2026-01-22 08:39:02,240][timm.models._builder][INFO] - Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
[2026-01-22 08:39:02,508][timm.models._hub][INFO] - [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
[2026-01-22 08:39:02,719][timm.models._builder][INFO] - Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
[2026-01-22 08:39:02,938][timm.models._hub][INFO] - [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Error executing job with overrides: ['experiment=rec_baseline_v1', '+trainer.fast_dev_run=True']
Error in call to target 'torch.optim.adam.Adam':
TypeError("Adam.__init__() missing 1 required positional argument: 'params'")
full_key: model.architectures.cfg.optimizer

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
