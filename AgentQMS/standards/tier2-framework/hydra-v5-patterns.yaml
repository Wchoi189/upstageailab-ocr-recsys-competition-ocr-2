ads_version: '1.0'
type: rule_set
agent: all
tier: 2
priority: critical
validates_with: AgentQMS/standards/schemas/compliance-checker.py
id: hydra-v5-patterns
name: Hydra "Domains First" v5.0 Configuration Patterns
description: Critical rules, design patterns, and failure modes for Hydra 1.3.2 configuration architecture
version: 5.0
hydra_version: 1.3.2
last_updated: '2026-01-18'

critical_rules:
  flattening_rule:
    name: The Flattening Rule
    priority: critical
    description: Files using `# @package _group_` MUST NOT contain a top-level key matching the folder name
    failure_mode: Double namespacing (e.g., `data.data.*` instead of `data.*`)
    examples:
      incorrect:
        file: configs/data/default.yaml
        content: |
          # @package _group_
          data:  # ❌ Creates data.data namespace
            train_num_samples: 1000
      correct:
        file: configs/data/default.yaml
        content: |
          # @package _group_
          train_num_samples: 1000  # ✅ Creates data.train_num_samples
    validation:
      check: Verify no top-level key matches parent folder name
      error_symptom: "Nested duplicate keys like data.data.*, train.train.*"

  absolute_interpolation_law:
    name: Absolute Interpolation Law
    priority: critical
    description: All cross-file references MUST use absolute paths from root namespace
    failure_mode: "InterpolationKeyError: 'key' not found"
    patterns:
      never_use:
        - "${train_transform}"
        - "${optimizer}"
        - "./data"
      always_use:
        - "${data.transforms.train_transform}"
        - "${train.optimizer.adamw}"
        - "${global.paths.data_dir}"
    validation:
      check: Search for relative interpolations without namespace prefix
      error_symptom: "InterpolationKeyError during config composition"

  package_directive_behavior:
    name: Package Directive Auto-Wrapping
    priority: critical
    description: "`# @package _group_` automatically wraps content in parent folder's namespace"
    mechanism: |
      Location: configs/data/default.yaml
      Package: # @package _group_
      Result: All keys placed under `data.*` namespace
    double_wrap_bug:
      symptom: "data.data.* namespace instead of data.*"
      cause: "File contains `data:` key + `# @package _group_`"
      fix: "Remove the `data:` wrapper key, keep only child keys"

component_placement_rules:
  neural_network_layers:
    location: model/architectures/
    rationale: Pure structural definition
    must_not_contain: [optimizer, loss, tokenizer]

  tokenizer:
    location: [domain/, data/]
    rationale: Data-dependent mapping, language-specific
    anti_pattern: Placing in model/architectures/ prevents language reuse

  loss_function:
    location: domain/
    rationale: Task-dependent logic
    anti_pattern: Placing in model/presets/ causes passive refactor cycle

  optimizer_lr:
    location: train/optimizer/
    rationale: Hardware and schedule configuration
    anti_pattern: Placing in model/presets/ creates invisible overrides

  transforms:
    location: data/transforms/
    rationale: Image alteration logic
    anti_pattern: Placing in data/datasets/ mixes source with logic

  dataset_paths:
    location: data/datasets/
    rationale: Source identity only (paths and metadata)
    must_only_contain: [image_path, annotation_path, lmdb_path, _target_]

design_patterns:
  atomic_architecture:
    name: Atomic Architecture Pattern
    principle: Model presets define ONLY neural network structure, not training logic
    example:
      file: configs/model/architectures/parseq.yaml
      content: |
        # @package _group_
        # v5.0 | Atomic Recognition Architecture
        _target_: ocr.domains.recognition.models.PARSeq
        backbone:
          _target_: ocr.core.models.encoder.TimmBackbone
          model_name: resnet18
          pretrained: true
        decoder:
          _target_: ocr.domains.recognition.models.decoder.PARSeqDecoder
          d_model: 512
          nhead: 8
          num_layers: 12
        max_len: 25
    anti_pattern:
      name: Passive Refactor Cycle
      symptom: Model presets include `optimizer:` or `loss:` keys
      problem: Optimizer override invisible to user, causes training inconsistencies
      fix: Remove all training logic from model/architectures/

  domain_injection:
    name: Domain Injection Pattern
    principle: Domain controller injects data-dependent components
    problem: Tokenizers in architecture files prevent language reuse
    solution:
      file: configs/domain/recognition.yaml
      content: |
        # @package _group_
        defaults:
          - /model/architectures: parseq
          - /data/datasets: recognition_canonical
          - _self_

        # Domain controller injects tokenizer
        model:
          tokenizer:
            _target_: ocr.domains.recognition.data.tokenizer.KoreanOCRTokenizer
            char_path: ${global.paths.root_dir}/ocr/data/charset.json
            max_len: 25

        # Loss function lives in domain (task-dependent)
        train:
          loss:
            _target_: torch.nn.CrossEntropyLoss
            ignore_index: 0

  callback_flattening:
    name: Callback Flattening Pattern
    failure_mode: "train.callbacks.early_stopping.early_stopping (double nesting)"
    incorrect:
      file: configs/train/callbacks/early_stopping.yaml
      content: |
        # @package _group_
        early_stopping:  # ❌ Creates double namespace
          _target_: lightning.pytorch.callbacks.EarlyStopping
          monitor: "val/hmean"
    correct:
      file: configs/train/callbacks/early_stopping.yaml
      content: |
        # @package _group_
        _target_: lightning.pytorch.callbacks.EarlyStopping
        monitor: "val/hmean"
        min_delta: 0.0
        patience: 5
        mode: "max"

  multi_logger_aliasing:
    name: Multi-Logger Aliasing Pattern
    problem: Multiple loggers without aliasing cause namespace collision
    syntax_breakdown:
      pattern: "wandb@_group_.wandb_logger"
      components:
        file_selection: wandb  # File to load from configs/train/logger/wandb.yaml
        package_directive: "@_group_"  # Place in parent namespace
        custom_alias: ".wandb_logger"  # Unique key for this logger
      result: "train.logger.wandb_logger.*"
    implementation:
      default_file:
        path: configs/train/logger/default.yaml
        content: |
          # @package _group_
          defaults:
            - wandb@_group_.wandb_logger  # Creates train.logger.wandb_logger
            - csv@_group_.csv_logger      # Creates train.logger.csv_logger
            - _self_
      logger_file:
        path: configs/train/logger/wandb.yaml
        content: |
          # @package _group_
          _target_: lightning.pytorch.loggers.WandbLogger
          project: "receipt-ocr-project"
          log_model: "all"
          save_dir: "${global.paths.wandb}"
          settings:
            offline: false

  dataset_source_identity:
    name: Dataset Source Identity Pattern
    principle: Dataset configs define ONLY paths and metadata, NOT transforms
    example:
      file: configs/data/datasets/canonical.yaml
      content: |
        # @package _group_
        # v5.0 | Dataset Source Paths ONLY
        dataset_base_path: "${global.paths.datasets_root}"

        train_dataset:
          _target_: ocr.data.datasets.ValidatedOCRDataset
          image_path: ${data.dataset_base_path}/images/train
          annotation_path: ${data.dataset_base_path}/jsons/train.json

        val_dataset:
          _target_: ocr.data.datasets.ValidatedOCRDataset
          image_path: ${data.dataset_base_path}/images_val_canonical
          annotation_path: ${data.dataset_base_path}/jsons/val.json
    note: Transforms injected separately via domain controller or data/transforms/

  domain_isolation:
    name: Domain Isolation Pattern
    principle: Domain controllers nullify irrelevant keys from other domains
    rationale: Prevents CUDA segfaults and logic leakage from unused domain configs
    example:
      file: configs/domain/recognition.yaml
      content: |
        # @package _group_
        defaults:
          - /model/presets: parseq
          - /data/datasets: recognition_canonical
          - /train/optimizer: adamw
          - /train/logger: default
          - /train/callbacks: default
          - _self_

        # Nullify detection-specific keys
        detection: null
        max_polygons: null
        shrink_ratio: null
        thresh_min: null
        thresh_max: null

        # Recognition-specific overrides
        recognition:
          max_label_length: 25
          charset: korean
          decode_mode: greedy

failure_modes:
  namespace_fragmentation:
    name: Namespace Fragmentation
    symptom: "data.data.train_num_samples instead of data.train_num_samples"
    root_cause: "File uses `# @package _group_` but contains top-level key matching folder"
    resolution: "Remove redundant wrapper key, flatten structure"
    validation_command: "python scripts/utils/show_config.py main domain=detection"

  orphaned_logic:
    name: Orphaned Logic Files
    symptom: "Config references non-existent paths like /dataloaders/default"
    example: "configs/data/datasets/db.yaml referencing deleted directories"
    resolution: "Delete orphaned files; move logic to domain controller or experiment configs"

  preprocessing_misplacement:
    name: Preprocessing in Wrong Location
    symptom: "configs/data/datasets/preprocessing.yaml exists"
    problem: "Preprocessing is a transformation, not a data source"
    resolution: "Move to configs/data/transforms/preprocessing.yaml"

  passive_refactor_cycle:
    name: Passive Refactor Cycle
    symptom: "Model presets include `- /train/optimizer: adamw` in defaults"
    problem: "Optimizer override invisible to user, causes training inconsistencies"
    resolution: "Remove all `optimizer:` and `loss:` from model/architectures/; manage in train/ or domain/"

  shadow_interpolations:
    name: Shadow Interpolations
    symptom: "${encoder_path} fails with 'key not found'"
    problem: "Relative interpolation without absolute root anchor"
    resolution: "Define in main.yaml or use absolute path ${global.paths.encoder_path}"

  interpolation_key_error:
    name: Interpolation Key Not Found
    symptom: "InterpolationKeyError: 'data.tokenizer' not found"
    root_cause: "Incorrect namespace resolution from package directive"
    common_causes:
      - "Using relative interpolation ${key} instead of ${namespace.key}"
      - "Package directive creating unexpected namespace nesting"
      - "Key defined in wrong config group"
    resolution: "Use absolute interpolation paths; verify namespace with show_config.py"

validation_checklist:
  flattening:
    - "All `# @package _group_` files have no top-level key matching folder name"
    - "Callbacks/loggers flattened (no redundant wrapper keys)"

  interpolation:
    - "All interpolations use absolute paths (${namespace.key})"
    - "All paths anchor to ${global.paths.*}"

  architecture_purity:
    - "Model architectures contain NO `optimizer:` or `loss:` keys"
    - "Tokenizers moved to domain controllers or data configs"

  component_placement:
    - "Dataset configs define ONLY paths, NOT transforms"
    - "Preprocessing moved to data/transforms/"

  multi_component:
    - "Multi-logger configs use aliasing (@_group_.alias)"

  domain_isolation:
    - "Domain controllers nullify irrelevant cross-domain keys"
    - "Orphaned files (invalid defaults) deleted"

validation_commands:
  verify_composition:
    detection: "python scripts/utils/show_config.py main domain=detection"
    recognition: "python scripts/utils/show_config.py main domain=recognition"

  check_errors:
    look_for:
      - "InterpolationKeyError: 'key' not found → Namespace mismatch"
      - "Nested duplicate keys (e.g., data.data.*) → Flattening violation"

verification_script:
  path: scripts/utils/verify_hydra_flattening.py
  purpose: Automated validation of v5.0 patterns
  checks:
    - Logger aliasing structure
    - Callback flattening
    - Interpolation resolution
  usage: "python scripts/utils/verify_hydra_flattening.py"

quick_reference:
  path: project_compass/history/sessions/20260118_013857_session-refactor-execution/hydra_quick_reference.md
  description: One-page cheat sheet for rapid pattern lookup

full_documentation:
  path: project_compass/history/sessions/20260118_013857_session-refactor-execution/hydra_config_patterns_knowledge_base.md
  description: Comprehensive technical reference with examples and failure modes

integration:
  standards_router:
    triggers:
      - config
      - yaml
      - hydra
      - configuration
      - namespace
      - interpolation
      - package directive
    auto_loaded: true

  companion_standards:
    - AgentQMS/standards/tier2-framework/hydra-configuration-architecture.yaml
    - AgentQMS/standards/tier2-framework/configuration-standards.yaml

compliance:
  agentqms_validated: '2026-01-18'
  source_session: 20260118_013857_session-refactor-execution
  research_document: research-hydra-specific-examples.md
  ai_optimized: true
