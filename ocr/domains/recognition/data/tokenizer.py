"""Korean OCR Tokenizer for character-level text recognition."""
import json
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


class KoreanOCRTokenizer:
    """
    Character-level tokenizer for Korean OCR.

    Special tokens:
        PAD (0): Padding
        BOS (1): Beginning of sequence
        EOS (2): End of sequence

    Usage:
        tokenizer = KoreanOCRTokenizer("ocr/data/charset.json", max_len=25)
        tokens = tokenizer.encode("안녕하세요")  # [1, ...]
        text = tokenizer.decode(tokens)
    """

    PAD = 0
    BOS = 1
    EOS = 2
    UNK = 3  # Unknown character

    SPECIAL_TOKENS = ["[PAD]", "[BOS]", "[EOS]", "[UNK]"]

    def __init__(self, charset_path: str | Path, max_len: int = 25):
        """
        Initialize tokenizer from charset JSON file.

        Args:
            charset_path: Path to charset.json (generated by extract_charset.py)
            max_len: Maximum sequence length (including BOS/EOS)
        """
        self.max_len = max_len

        # Load charset
        with open(charset_path, encoding="utf-8") as f:
            data = json.load(f)

        charset = data["charset"]

        # Build vocab: specials + charset
        self.id_to_char: dict[int, str] = {}
        self.char_to_id: dict[str, int] = {}

        # Add special tokens
        for i, token in enumerate(self.SPECIAL_TOKENS):
            self.id_to_char[i] = token
            self.char_to_id[token] = i

        # Add charset characters
        for i, char in enumerate(charset, start=len(self.SPECIAL_TOKENS)):
            self.id_to_char[i] = char
            self.char_to_id[char] = i

        logger.info(
            "Loaded tokenizer: %d chars, vocab_size=%d, max_len=%d",
            len(charset), self.vocab_size, max_len
        )

    @property
    def vocab_size(self) -> int:
        """Total vocabulary size including special tokens."""
        return len(self.id_to_char)

    def encode(self, text: str, add_special_tokens: bool = True) -> list[int]:
        """
        Encode text to token IDs.

        Args:
            text: Input text string
            add_special_tokens: If True, add BOS at start and EOS at end

        Returns:
            List of token IDs, padded to max_len
        """
        tokens = []

        if add_special_tokens:
            tokens.append(self.BOS)

        for char in text:
            token_id = self.char_to_id.get(char, self.UNK)
            tokens.append(token_id)

        if add_special_tokens:
            tokens.append(self.EOS)

        # Truncate if needed
        if len(tokens) > self.max_len:
            tokens = tokens[:self.max_len - 1] + [self.EOS] if add_special_tokens else tokens[:self.max_len]

        # Pad to max_len
        while len(tokens) < self.max_len:
            tokens.append(self.PAD)

        return tokens

    def decode(self, tokens: list[int], skip_special_tokens: bool = True) -> str:
        """
        Decode token IDs back to text.

        Args:
            tokens: List of token IDs
            skip_special_tokens: If True, skip PAD/BOS/EOS/UNK

        Returns:
            Decoded text string
        """
        chars = []
        special_ids = {self.PAD, self.BOS, self.EOS, self.UNK}

        for token_id in tokens:
            if skip_special_tokens and token_id in special_ids:
                if token_id == self.EOS:
                    break  # Stop at EOS
                continue

            char = self.id_to_char.get(token_id, "")
            if char and not char.startswith("["):  # Skip special token strings
                chars.append(char)

        return "".join(chars)

    def batch_encode(self, texts: list[str], add_special_tokens: bool = True) -> list[list[int]]:
        """Encode a batch of texts."""
        return [self.encode(text, add_special_tokens) for text in texts]

    def batch_decode(self, token_batch: list[list[int]], skip_special_tokens: bool = True) -> list[str]:
        """Decode a batch of token sequences."""
        return [self.decode(tokens, skip_special_tokens) for tokens in token_batch]
