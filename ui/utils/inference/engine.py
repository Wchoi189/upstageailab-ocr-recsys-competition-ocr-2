from __future__ import annotations

"""High-level inference engine orchestration."""

import logging
import re
import threading
import time
import warnings
from collections.abc import Callable
from pathlib import Path
from typing import Any

import cv2
import numpy as np
from PIL import Image

# Suppress Pydantic v2 configuration warnings from dependencies
warnings.filterwarnings("ignore", message="Valid config keys have changed in V2:", category=UserWarning)
warnings.filterwarnings("ignore", message="'allow_population_by_field_name' has been renamed to 'validate_by_name'", category=UserWarning)

from ocr.utils.orientation import normalize_pil_image, orientation_requires_rotation, remap_polygons
from ocr.utils.orientation_constants import ORIENTATION_INVERSE_INT

from .config_loader import ModelConfigBundle, PostprocessSettings, load_model_config, resolve_config_path
from .dependencies import OCR_MODULES_AVAILABLE, PROJECT_ROOT, torch
from ocr.utils.path_utils import get_path_resolver
from .model_loader import instantiate_model, load_checkpoint, load_state_dict
from .postprocess import decode_polygons_with_head, fallback_postprocess
from .preprocess import build_transform, preprocess_image
from .utils import generate_mock_predictions
from .utils import get_available_checkpoints as scan_checkpoints

LOGGER = logging.getLogger(__name__)

_POLYGON_TOKEN_PATTERN = re.compile(r"-?\d+(?:\.\d+)?")


def _run_with_timeout(func: Callable, timeout_seconds: int = 30) -> Any:
    """Run a function with a timeout using threading (thread-safe for Streamlit).

    This implementation uses threading instead of signal.signal() to be compatible
    with Streamlit's threading model. signal.signal() only works in the main thread,
    but Streamlit runs in a separate thread.

    Args:
        func: Function to execute
        timeout_seconds: Timeout in seconds

    Returns:
        Result of the function

    Raises:
        TimeoutError: If function execution exceeds timeout
        Exception: Any exception raised by the function
    """
    result = [None]
    exception = [None]

    def _wrapper():
        try:
            result[0] = func()
        except Exception as e:
            exception[0] = e

    thread = threading.Thread(target=_wrapper)
    thread.daemon = True
    thread.start()
    thread.join(timeout=timeout_seconds)

    if thread.is_alive():
        # Thread is still running - timeout occurred
        LOGGER.error(f"Function timed out after {timeout_seconds} seconds")
        raise TimeoutError(f"Inference operation timed out after {timeout_seconds} seconds")

    if exception[0] is not None:
        raise exception[0]

    return result[0]


class InferenceEngine:
    """OCR Inference Engine for real-time predictions."""

    def __init__(self) -> None:
        self.model = None
        self.trainer = None
        self.config: Any | None = None
        self.device = "cuda" if torch is not None and torch.cuda.is_available() else "cpu"

        self._config_bundle: ModelConfigBundle | None = None
        self._transform = None
        self._postprocess_settings: PostprocessSettings | None = None
        LOGGER.info("Using device: %s", self.device)

    # Public API ---------------------------------------------------------
    def load_model(self, checkpoint_path: str, config_path: str | None = None) -> bool:
        if not OCR_MODULES_AVAILABLE:
            LOGGER.error("OCR modules are not installed. Cannot load a real model.")
            return False

        # Include configs directory in search paths for config files
        # Use path resolver for config directory (supports environment variable override)
        resolver = get_path_resolver()
        search_dirs = (resolver.config.config_dir,)
        resolved_config = resolve_config_path(checkpoint_path, config_path, search_dirs)
        if resolved_config is None:
            LOGGER.error("Could not find a valid config file for checkpoint: %s", checkpoint_path)
            return False

        LOGGER.info("Loading model from checkpoint: %s", checkpoint_path)
        bundle = load_model_config(resolved_config)
        self._apply_config_bundle(bundle)

        model_config = getattr(bundle.raw_config, "model", None)
        if model_config is None:
            # Fallback: Try to extract model config from root level (for Hydra configs with defaults)
            LOGGER.warning("Configuration missing direct 'model' section, trying root level extraction: %s", resolved_config)
            model_config = bundle.raw_config
            # Check if this config has model-like attributes
            model_attrs = ["architecture", "encoder", "decoder", "head"]
            if not any(hasattr(model_config, attr) for attr in model_attrs):
                LOGGER.error("Configuration has neither 'model' section nor model attributes at root level: %s", resolved_config)
                return False

        try:
            model = instantiate_model(model_config)
        except Exception:  # noqa: BLE001
            LOGGER.exception("Failed to instantiate model from config %s", resolved_config)
            return False

        checkpoint = load_checkpoint(checkpoint_path, self.device)
        if checkpoint is None:
            LOGGER.error("Failed to load checkpoint %s", checkpoint_path)
            return False

        if not load_state_dict(model, checkpoint):
            LOGGER.error("Failed to load state dictionary for checkpoint %s", checkpoint_path)
            return False

        self.model = model.to(self.device)
        assert self.model is not None  # Model loading succeeded
        self.model.eval()
        self.config = bundle.raw_config
        return True

    def update_postprocessor_params(
        self,
        binarization_thresh: float | None = None,
        box_thresh: float | None = None,
        max_candidates: int | None = None,
        min_detection_size: int | None = None,
    ) -> None:
        if self.model is None:
            LOGGER.warning("Model not loaded, cannot update postprocessor parameters.")
            return

        settings = self._postprocess_settings
        if settings is None:
            return

        updated_settings = PostprocessSettings(
            binarization_thresh=binarization_thresh if binarization_thresh is not None else settings.binarization_thresh,
            box_thresh=box_thresh if box_thresh is not None else settings.box_thresh,
            max_candidates=int(max_candidates) if max_candidates is not None else settings.max_candidates,
            min_detection_size=int(min_detection_size) if min_detection_size is not None else settings.min_detection_size,
        )
        self._postprocess_settings = updated_settings
        if self._config_bundle is not None:
            self._config_bundle = ModelConfigBundle(
                raw_config=self._config_bundle.raw_config,
                preprocess=self._config_bundle.preprocess,
                postprocess=updated_settings,
            )

        head = getattr(self.model, "head", None)
        postprocess = getattr(head, "postprocess", None)
        if postprocess is None:
            LOGGER.info("Inference model lacks a postprocess module; using engine fallbacks.")
            return

        if hasattr(postprocess, "thresh") and binarization_thresh is not None:
            postprocess.thresh = binarization_thresh
        if hasattr(postprocess, "box_thresh") and box_thresh is not None:
            postprocess.box_thresh = box_thresh
        if hasattr(postprocess, "max_candidates") and max_candidates is not None:
            postprocess.max_candidates = int(max_candidates)
        if hasattr(postprocess, "min_size") and min_detection_size is not None:
            postprocess.min_size = int(min_detection_size)

    def predict_array(
        self,
        image_array: np.ndarray,
        binarization_thresh: float | None = None,
        box_thresh: float | None = None,
        max_candidates: int | None = None,
        min_detection_size: int | None = None,
    ) -> dict[str, Any] | None:
        """Predict on an image array (numpy) directly.

        New optimized path that accepts numpy arrays directly without file I/O.
        Eliminates tempfile overhead from the inference pipeline.

        Args:
            image_array: Image as numpy array (RGB or BGR format)
            binarization_thresh: Binarization threshold override
            box_thresh: Box threshold override
            max_candidates: Max candidates override
            min_detection_size: Minimum detection size override

        Returns:
            Predictions dict with 'polygons', 'texts', 'confidences' keys

        See: artifacts/implementation_plans/2025-11-12_plan-004-revised-inference-consolidation.md
        """
        LOGGER.info("Starting prediction for numpy array with shape: %s", image_array.shape)

        if self.model is None:
            LOGGER.warning("Model not loaded. Returning mock predictions.")
            return generate_mock_predictions()

        if any(value is not None for value in (binarization_thresh, box_thresh, max_candidates, min_detection_size)):
            self.update_postprocessor_params(
                binarization_thresh=binarization_thresh,
                box_thresh=box_thresh,
                max_candidates=max_candidates,
                min_detection_size=min_detection_size,
            )

        # Assume image is already in BGR format (OpenCV standard)
        # If RGB, caller should convert before calling this method
        image = image_array

        LOGGER.info(f"Image array received successfully. Shape: {image.shape}")

        return self._predict_from_array(image)

    def predict_image(
        self,
        image_path: str,
        binarization_thresh: float | None = None,
        box_thresh: float | None = None,
        max_candidates: int | None = None,
        min_detection_size: int | None = None,
    ) -> dict[str, Any] | None:
        """Predict on an image file (legacy file path-based API).

        Kept for backward compatibility. New code should use predict_array() directly.

        Args:
            image_path: Path to image file
            binarization_thresh: Binarization threshold override
            box_thresh: Box threshold override
            max_candidates: Max candidates override
            min_detection_size: Minimum detection size override

        Returns:
            Predictions dict with 'polygons', 'texts', 'confidences' keys
        """
        LOGGER.info(f"Starting prediction for image: {image_path}")

        if self.model is None:
            LOGGER.warning("Model not loaded. Returning mock predictions.")
            return generate_mock_predictions()

        if any(value is not None for value in (binarization_thresh, box_thresh, max_candidates, min_detection_size)):
            self.update_postprocessor_params(
                binarization_thresh=binarization_thresh,
                box_thresh=box_thresh,
                max_candidates=max_candidates,
                min_detection_size=min_detection_size,
            )

        orientation = 1
        canonical_width = 0
        canonical_height = 0
        raw_width = 0
        raw_height = 0

        try:
            with Image.open(image_path) as pil_image:
                raw_width, raw_height = pil_image.size
                normalized_image, orientation = normalize_pil_image(pil_image)

                rgb_image = normalized_image
                if normalized_image.mode != "RGB":
                    rgb_image = normalized_image.convert("RGB")

                image_array = np.asarray(rgb_image)
                canonical_height, canonical_width = image_array.shape[:2]
                image = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)

                if rgb_image is not normalized_image:
                    rgb_image.close()
                if normalized_image is not pil_image:
                    normalized_image.close()
        except OSError:
            LOGGER.error("Failed to read image at path: %s", image_path)
            return None

        LOGGER.info(f"Image loaded successfully. Shape: {image.shape}, Orientation: {orientation}")

        return self._predict_from_array(image)

    def _predict_from_array(self, image: np.ndarray) -> dict[str, Any] | None:
        """Internal method: shared prediction logic for both file and array paths.

        Args:
            image: Image as BGR numpy array

        Returns:
            Predictions dict or None on failure
        """
        bundle = self._config_bundle
        if bundle is None:
            LOGGER.error("Model configuration bundle missing; load_model must be called first.")
            return None

        if self._transform is None:
            self._transform = build_transform(bundle.preprocess)

        try:
            batch = preprocess_image(image, self._transform)
        except Exception:  # noqa: BLE001
            LOGGER.exception("Failed to preprocess image")
            return None

        LOGGER.info(f"Image preprocessing completed. Batch shape: {batch.shape if hasattr(batch, 'shape') else 'unknown'}")

        if torch is None:
            LOGGER.error("Torch is not available to run inference.")
            return None

        LOGGER.info("Starting model inference...")
        start_time = time.time()
        try:
            if self.model is None:
                raise RuntimeError("Model is not loaded")

            # Direct inference without timeout wrapper to avoid threading issues in Streamlit
            # Streamlit has its own timeout mechanism, and nested threading can cause crashes
            with torch.no_grad():
                predictions = self.model(return_loss=False, images=batch.to(self.device))

            inference_time = time.time() - start_time
            LOGGER.info(f"Model inference completed in {inference_time:.2f} seconds")
        except Exception as e:
            LOGGER.exception(f"Model inference failed: {e}")
            return None

        decoded = decode_polygons_with_head(self.model, batch, predictions, image.shape)
        if decoded is not None:
            LOGGER.info("Primary decoding successful")
            return decoded

        LOGGER.info("Primary decoding failed, trying fallback postprocessing...")
        try:
            result = fallback_postprocess(predictions, image.shape, bundle.postprocess)
        except Exception:  # noqa: BLE001
            LOGGER.exception("Error in post-processing for image with shape %s", image.shape)
            return generate_mock_predictions()

        LOGGER.info("Fallback postprocessing completed")
        return result

    # Internal helpers ---------------------------------------------------
    def _apply_config_bundle(self, bundle: ModelConfigBundle) -> None:
        self._config_bundle = bundle
        self._postprocess_settings = bundle.postprocess
        self._transform = None

    @staticmethod
    def _remap_predictions_if_needed(
        result: dict[str, Any],
        orientation: int,
        canonical_width: int,
        canonical_height: int,
        raw_width: int,
        raw_height: int,
    ) -> dict[str, Any]:
        if not result:
            return result

        if not orientation_requires_rotation(orientation):
            return result

        polygons_str = result.get("polygons")
        if not polygons_str:
            return result

        inverse_orientation = ORIENTATION_INVERSE_INT.get(orientation, 1)
        if inverse_orientation == 1:
            return result

        remapped_polygons = []
        for polygon_entry in polygons_str.split("|"):
            tokens = _POLYGON_TOKEN_PATTERN.findall(polygon_entry)
            if len(tokens) < 8 or len(tokens) % 2 != 0:
                continue
            try:
                coords = np.array([float(value) for value in tokens], dtype=np.float32).reshape(-1, 2)
            except ValueError:
                continue
            remapped_polygons.append(coords)

        if not remapped_polygons:
            return result

        transformed = remap_polygons(remapped_polygons, raw_width, raw_height, inverse_orientation)
        serialised: list[str] = []
        for polygon in transformed:
            flat = polygon.reshape(-1)
            # Competition format uses space-separated coordinates, not commas
            serialised.append(" ".join(str(int(round(value))) for value in flat))

        updated = dict(result)
        updated["polygons"] = "|".join(serialised)
        return updated


def run_inference_on_image(
    image_path: str | np.ndarray,
    checkpoint_path: str,
    config_path: str | None = None,
    binarization_thresh: float | None = None,
    box_thresh: float | None = None,
    max_candidates: int | None = None,
    min_detection_size: int | None = None,
) -> dict[str, Any] | None:
    """Run inference on an image (file path or numpy array).

    Supports both legacy file path API and new numpy array API for eliminating
    tempfile overhead.

    Args:
        image_path: Either file path (str) or numpy array (optimized path)
        checkpoint_path: Path to model checkpoint
        config_path: Optional path to config file
        binarization_thresh: Binarization threshold override
        box_thresh: Box threshold override
        max_candidates: Max candidates override
        min_detection_size: Minimum detection size override

    Returns:
        Predictions dict with 'polygons', 'texts', 'confidences' keys

    See: artifacts/implementation_plans/2025-11-12_plan-004-revised-inference-consolidation.md
    """
    engine = InferenceEngine()
    if not engine.load_model(checkpoint_path, config_path):
        LOGGER.error("Failed to load model in convenience function.")
        return None

    # NEW: Support numpy array input (eliminates tempfile overhead)
    if isinstance(image_path, np.ndarray):
        return engine.predict_array(
            image_array=image_path,
            binarization_thresh=binarization_thresh,
            box_thresh=box_thresh,
            max_candidates=max_candidates,
            min_detection_size=min_detection_size,
        )

    # LEGACY: Support file path input (backward compatible)
    return engine.predict_image(
        image_path=image_path,
        binarization_thresh=binarization_thresh,
        box_thresh=box_thresh,
        max_candidates=max_candidates,
        min_detection_size=min_detection_size,
    )


def get_available_checkpoints() -> list[str]:
    return scan_checkpoints()
