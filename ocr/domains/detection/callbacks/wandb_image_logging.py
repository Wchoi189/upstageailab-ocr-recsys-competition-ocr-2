from __future__ import annotations

from collections.abc import Iterable, Sequence
from pathlib import Path
from typing import Any

import lightning.pytorch as pl
import numpy as np
from PIL import Image

from ocr.core.lightning.processors.image_processor import ImageProcessor
from ocr.core.utils.config_utils import is_config
from ocr.core.utils.geometry_utils import apply_padding_offset_to_polygons, compute_padding_offsets
from ocr.core.utils.orientation import normalize_pil_image, remap_polygons
from ocr.domains.detection.utils.polygons import ensure_polygon_array
from ocr.domains.detection.callbacks.wandb import log_validation_images


class WandbImageLoggingCallback(pl.Callback):
    """Callback to log validation images with bounding boxes to Weights & Biases."""

    def __init__(self, log_every_n_epochs: int = 1, max_images: int = 8):
        self.log_every_n_epochs = log_every_n_epochs
        self.max_images = max_images

    def on_validation_epoch_end(self, trainer, pl_module):
        # Only log every N epochs to avoid too much data
        if trainer.current_epoch % self.log_every_n_epochs != 0:
            return

        # Collect validation images, ground truth, and predictions for logging
        if not hasattr(pl_module, "validation_step_outputs") or not pl_module.validation_step_outputs:
            return

        # Get the validation dataset for ground truth
        if not hasattr(pl_module, "dataset") or not is_config(pl_module.dataset) or "val" not in pl_module.dataset:
            return

        val_dataset = pl_module.dataset["val"]  # type: ignore

        # Handle Subset datasets - get the underlying dataset
        if hasattr(val_dataset, "dataset"):
            val_dataset = val_dataset.dataset  # type: ignore

        # Prepare data for logging
        images = []
        gt_bboxes = []
        pred_bboxes = []
        filenames = []

        # Collect up to max_images samples
        count = 0
        for filename, pred_data in list(pl_module.validation_step_outputs.items())[: self.max_images]:  # type: ignore
            entry = pred_data if is_config(pred_data) else {"boxes": pred_data}
            pred_boxes = entry.get("boxes", [])
            orientation_hint = entry.get("orientation", 1)
            raw_size_hint = entry.get("raw_size")
            metadata = self._normalize_metadata(entry.get("metadata"))

            if metadata:
                if "orientation" in metadata and metadata["orientation"] is not None:
                    try:
                        orientation_hint = int(metadata["orientation"])
                    except (TypeError, ValueError):
                        pass
                if "raw_size" in metadata and metadata["raw_size"] is not None:
                    raw_size_hint = metadata["raw_size"]

            if not hasattr(val_dataset, "anns") or filename not in val_dataset.anns:  # type: ignore
                continue

            # Get ground truth boxes
            gt_boxes = val_dataset.anns[filename]  # type: ignore
            gt_quads = self._normalise_polygons(gt_boxes)
            pred_quads = self._normalise_polygons(pred_boxes)

            # BUG-20251116-001: Check if transformed_image is available (640x640 tensor)
            transformed_image = entry.get("transformed_image")
            using_transformed_image = transformed_image is not None

            # Get image - prefer transformed_image if available, otherwise load from disk
            try:
                if using_transformed_image:
                    assert transformed_image is not None, "transformed_image should not be None when using_transformed_image is True"
                    # Use transformed_image (640x640, CHW, normalized with ImageNet stats)
                    # Convert tensor to PIL Image with proper denormalization
                    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
                    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)
                    image = ImageProcessor.tensor_to_pil_image(transformed_image, mean=mean, std=std)
                    # Image is now 640x640 RGB PIL Image

                    # BUG-20251116-001: Handle GT polygon orientation remapping if needed
                    # GT polygons might be in raw coordinates and need remapping to canonical
                    polygon_frame = metadata.get("polygon_frame") if metadata else None
                    if gt_quads and polygon_frame != "canonical" and orientation_hint != 1:
                        # Remap GT polygons to canonical frame if needed
                        if raw_size_hint and len(raw_size_hint) == 2:
                            raw_w, raw_h = raw_size_hint
                            if raw_w > 0 and raw_h > 0:
                                gt_quads = remap_polygons(gt_quads, raw_w, raw_h, orientation_hint)

                    # BUG-20251116-001: Both GT and pred polygons are in original/canonical coordinates
                    # We need to scale them to match the 640x640 transformed image
                    # NOTE: canonical_size in batch is already 640x640 (after transforms), so we use raw_size
                    # The scale factor is computed from LongestMaxSize(640) transform: 640.0 / max(raw_w, raw_h)
                    if raw_size_hint and len(raw_size_hint) == 2:
                        raw_w, raw_h = raw_size_hint
                        if raw_w > 0 and raw_h > 0:
                            # Scale factor: LongestMaxSize scales longest side to 640
                            # Use max dimension to compute scale (orientation doesn't matter for max)
                            scale = 640.0 / max(raw_w, raw_h)

                            # Scale both GT and pred polygons
                            gt_quads = self._scale_polygons(gt_quads, scale)
                            pred_quads = self._scale_polygons(pred_quads, scale)

                            # BUG-20251116-001: Apply padding offset to polygons to match transformed image
                            # Transform config now uses position: "top_left" (uncommented)
                            # For models trained before this change, they may have used "center" padding
                            # TODO: Read padding position from transform metadata or make configurable
                            padding_position = "top_left"  # Matches configs/transforms/base.yaml
                            pad_x, pad_y = compute_padding_offsets(
                                (raw_w, raw_h),
                                target_size=640,
                                position=padding_position,
                            )

                            # Apply padding offset to GT polygons
                            if pad_x != 0 or pad_y != 0:
                                gt_quads = apply_padding_offset_to_polygons(gt_quads, pad_x, pad_y)

                            # BUG-20251116-001: Root cause fixed - inverse_matrix now computed correctly
                            # with proper padding position in transforms.py, so no compensation needed here
                else:
                    # Fallback: Get image directly from filesystem (similar to dataset loading)
                    image_path = self._resolve_image_path(entry, metadata, val_dataset, filename)
                    with Image.open(image_path) as pil_image:
                        raw_width, raw_height = pil_image.size
                        normalized_image, orientation = normalize_pil_image(pil_image)

                        if normalized_image.mode != "RGB":
                            image = normalized_image.convert("RGB")
                            normalized_image.close()
                        else:
                            image = normalized_image.copy()
                            normalized_image.close()

                    polygon_frame = metadata.get("polygon_frame") if metadata else None
                    if gt_quads:
                        if polygon_frame == "canonical":
                            pass
                        elif orientation != 1:
                            gt_quads = remap_polygons(gt_quads, raw_width, raw_height, orientation)
                        elif orientation_hint != 1:
                            hint_width, hint_height = raw_size_hint or (raw_width, raw_height)
                            gt_quads = remap_polygons(gt_quads, hint_width, hint_height, orientation_hint)

                gt_quads = self._postprocess_polygons(gt_quads, image.size)
                pred_quads = self._postprocess_polygons(pred_quads, image.size)

                images.append(image)
                gt_bboxes.append(gt_quads)
                pred_bboxes.append(pred_quads)
                filenames.append((filename, image.size[0], image.size[1]))  # (filename, width, height)
                count += 1

                if count >= self.max_images:
                    break
            except Exception as e:
                # If we can't get the image, skip this sample
                print(f"Warning: Failed to load image {filename}: {e}")
                continue

        # Log images with bounding boxes if we have data
        if images and gt_bboxes and pred_bboxes:
            try:
                log_validation_images(
                    images=images,
                    gt_bboxes=gt_bboxes,
                    pred_bboxes=pred_bboxes,
                    epoch=trainer.current_epoch,
                    limit=self.max_images,
                    filenames=filenames,
                )
            except Exception as e:
                # Log error but don't crash training
                print(f"Warning: Failed to log validation images to WandB: {e}")

    @staticmethod
    def _normalise_polygons(polygons: Sequence | Iterable | None) -> list[np.ndarray]:
        if not polygons:
            return []

        normalised: list[np.ndarray] = []
        for polygon in polygons:  # type: ignore[arg-type]
            try:
                polygon_array = ensure_polygon_array(np.asarray(polygon))  # type: ignore[arg-type]
            except ValueError as exc:
                print(f"Warning: Skipping polygon due to shape error: {exc}")
                continue

            if polygon_array is None or polygon_array.size == 0:
                continue

            normalised.append(np.array(polygon_array, copy=True))

        return normalised

    @staticmethod
    def _scale_polygons(polygons: list[np.ndarray], scale: float) -> list[np.ndarray]:
        """Scale polygon coordinates by a scale factor.

        Args:
            polygons: List of polygon arrays, each of shape (N, 2)
            scale: Scale factor to apply

        Returns:
            List of scaled polygon arrays
        """
        if not polygons or scale == 1.0:
            return polygons

        scaled = []
        for polygon in polygons:
            if polygon.size == 0:
                continue
            scaled_polygon = polygon * scale
            scaled.append(scaled_polygon)

        return scaled

    @staticmethod
    def _postprocess_polygons(polygons: Sequence[np.ndarray] | None, image_size: tuple[int, int]) -> list[np.ndarray]:
        """Filter degenerate polygons using shared validators from polygon_utils."""
        from ocr.domains.detection.utils.polygons import has_duplicate_consecutive_points

        if not polygons:
            return []

        # Filter out degenerate polygons (empty or with duplicate consecutive points)
        processed = [
            np.array(polygon, copy=True) for polygon in polygons if polygon.size > 0 and not has_duplicate_consecutive_points(polygon)
        ]

        width, height = image_size
        return processed

    @staticmethod
    def _normalize_metadata(metadata: Any) -> dict[str, Any] | None:
        if metadata is None:
            return None
        if hasattr(metadata, "model_dump"):
            metadata = metadata.model_dump()
        elif not is_config(metadata):
            try:
                metadata = dict(metadata)
            except Exception:  # noqa: BLE001
                return None

        normalized: dict[str, Any] = {}
        for key, value in metadata.items():
            if key == "path" and value is not None:
                normalized[key] = Path(value)
            elif key in {"raw_size", "canonical_size"} and value is not None:
                normalized[key] = WandbImageLoggingCallback._ensure_size_tuple(value)
            elif key == "orientation" and value is not None:
                try:
                    normalized[key] = int(value)
                except (TypeError, ValueError):  # noqa: BLE001
                    continue
            else:
                normalized[key] = value

        return normalized

    @staticmethod
    def _ensure_size_tuple(value: Any) -> tuple[int, int] | None:
        if value is None:
            return None
        if isinstance(value, tuple) and len(value) == 2:
            return int(value[0]), int(value[1])
        if isinstance(value, list) and len(value) == 2:
            return int(value[0]), int(value[1])
        try:
            width, height = value  # type: ignore[misc]
            return int(width), int(height)
        except Exception:  # noqa: BLE001
            return None

    @staticmethod
    def _resolve_image_path(entry: dict[str, Any], metadata: dict[str, Any] | None, dataset: Any, filename: str) -> Path:
        candidates: list[Any] = [entry.get("image_path")]
        if metadata is not None:
            candidates.append(metadata.get("path"))

        for candidate in candidates:
            if candidate is None:
                continue
            candidate_path = Path(candidate)
            if candidate_path.is_absolute():
                return candidate_path
            if hasattr(dataset, "image_path"):
                base_path = dataset.image_path  # type: ignore[attr-defined]
                return Path(base_path) / candidate_path

        # Fallback to dataset root
        if hasattr(dataset, "image_path"):
            return Path(dataset.image_path) / filename  # type: ignore[attr-defined]

        return Path(filename)
